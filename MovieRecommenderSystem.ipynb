{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка данных\n",
    "Для валидации было выбрано 25% последних рекомендаций для каждого пользователя. Если эти 25% включают более 100 фильмов, то выбираются только последние 100. Иначе единичные пользователи, имеющие очень большое число рецензий, будут вносить крайне значительный вклад в валидацию, чего бы не хотелось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part of data that is taken for validation: 0.2011420049826345\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_movie_sequences(data, user_val_part=0.25, max_user_val_movies=100):\n",
    "    # make train and validation sequences based on order of timestamp. For validation we take 25% of user's ratings.\n",
    "    # for user that have too many ratings we take only \"max_user_val_movies\"\n",
    "    train_user_ratings, val_user_ratings = [], []\n",
    "    train_user_movie_seq, val_user_movie_seq = [], []\n",
    "    total_n_val = 0\n",
    "    for _user_id, user_movies in data.groupby('userId'):\n",
    "        user_movies = user_movies.sort_values(by=['timestamp'])\n",
    "        user_ratings = user_movies['rating'].values.tolist()\n",
    "        user_movie_seq = user_movies['movieId'].values.tolist()\n",
    "\n",
    "        n_val = min(int(len(user_movie_seq) * user_val_part), max_user_val_movies)\n",
    "        total_n_val += n_val\n",
    "\n",
    "        train_user_ratings.append(user_ratings[:n_val])\n",
    "        val_user_ratings.append(user_ratings[n_val:])\n",
    "\n",
    "        train_user_movie_seq.append(user_movie_seq[:n_val])\n",
    "        val_user_movie_seq.append(user_movie_seq[n_val:])\n",
    "        \n",
    "    print('part of data that is taken for validation:', total_n_val / len(data))    \n",
    "    return train_user_ratings, val_user_ratings, train_user_movie_seq, val_user_movie_seq\n",
    "\n",
    "def encode_movieId(data):\n",
    "    # map movie id to new index based on frequency of this movie's occurrence in dataset:\n",
    "    # most frequent movie will be encoded with index 0, and less frequent will have max index value\n",
    "    cnts = data['movieId'].value_counts()\n",
    "    ind_range = [i for i, _x in enumerate(cnts)]\n",
    "    movie_id2ind = dict(zip(cnts.index, ind_range))\n",
    "    ind2movie_id = dict(zip(ind_range, cnts.index))\n",
    "    data['movieId'] = data['movieId'].map(movie_id2ind)\n",
    "    return data, ind2movie_id, len(cnts)\n",
    "\n",
    "def get_data():\n",
    "    data = pd.read_csv('data/ratings.csv')\n",
    "    data, ind2movie_id, n_movies = encode_movieId(data)\n",
    "    splitted_prepared_data = get_movie_sequences(data)\n",
    "    return (*splitted_prepared_data, ind2movie_id, n_movies)\n",
    "\n",
    "train_user_ratings, val_user_ratings, train_user_movie_seq, val_user_movie_seq, _ind2movie_id, n_movies = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie2vec\n",
    "с помощью skip-gram negative sampling (SGNS) получаем матрицу похожести фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0/200000, loss=0.6951915621757507\n",
      "Iteration 100/200000, loss=0.7001964449882507\n",
      "Iteration 200/200000, loss=0.7202941179275513\n",
      "Iteration 300/200000, loss=0.7075939178466797\n",
      "Iteration 400/200000, loss=0.6857410073280334\n",
      "Iteration 500/200000, loss=0.7215298414230347\n",
      "Iteration 600/200000, loss=0.7082899808883667\n",
      "Iteration 700/200000, loss=0.6965871453285217\n",
      "Iteration 800/200000, loss=0.6947771906852722\n",
      "Iteration 900/200000, loss=0.6861074566841125\n",
      "Iteration 1000/200000, loss=0.6703883409500122\n",
      "Iteration 1100/200000, loss=0.686747670173645\n",
      "Iteration 1200/200000, loss=0.7007666826248169\n",
      "Iteration 1300/200000, loss=0.7109870910644531\n",
      "Iteration 1400/200000, loss=0.6915712356567383\n",
      "Iteration 1500/200000, loss=0.6862387657165527\n",
      "Iteration 1600/200000, loss=0.6964664459228516\n",
      "Iteration 1700/200000, loss=0.6839592456817627\n",
      "Iteration 1800/200000, loss=0.6990758180618286\n",
      "Iteration 1900/200000, loss=0.6977675557136536\n",
      "Iteration 2000/200000, loss=0.6762588024139404\n",
      "Iteration 2100/200000, loss=0.7022560238838196\n",
      "Iteration 2200/200000, loss=0.7095745205879211\n",
      "Iteration 2300/200000, loss=0.7001746892929077\n",
      "Iteration 2400/200000, loss=0.7085963487625122\n",
      "Iteration 2500/200000, loss=0.6951248049736023\n",
      "Iteration 2600/200000, loss=0.6886642575263977\n",
      "Iteration 2700/200000, loss=0.7024100422859192\n",
      "Iteration 2800/200000, loss=0.7208511829376221\n",
      "Iteration 2900/200000, loss=0.6902056336402893\n",
      "Iteration 3000/200000, loss=0.6635953187942505\n",
      "Iteration 3100/200000, loss=0.6708693504333496\n",
      "Iteration 3200/200000, loss=0.6934481263160706\n",
      "Iteration 3300/200000, loss=0.6928123831748962\n",
      "Iteration 3400/200000, loss=0.6865162253379822\n",
      "Iteration 3500/200000, loss=0.682975172996521\n",
      "Iteration 3600/200000, loss=0.7039726376533508\n",
      "Iteration 3700/200000, loss=0.6758729219436646\n",
      "Iteration 3800/200000, loss=0.6911865472793579\n",
      "Iteration 3900/200000, loss=0.6803502440452576\n",
      "Iteration 4000/200000, loss=0.692849338054657\n",
      "Iteration 4100/200000, loss=0.7031867504119873\n",
      "Iteration 4200/200000, loss=0.6810559034347534\n",
      "Iteration 4300/200000, loss=0.6945202350616455\n",
      "Iteration 4400/200000, loss=0.7077922821044922\n",
      "Iteration 4500/200000, loss=0.7160572409629822\n",
      "Iteration 4600/200000, loss=0.6825610995292664\n",
      "Iteration 4700/200000, loss=0.669506847858429\n",
      "Iteration 4800/200000, loss=0.7118514180183411\n",
      "Iteration 4900/200000, loss=0.7405561208724976\n",
      "Iteration 5000/200000, loss=0.6879825592041016\n",
      "Iteration 5100/200000, loss=0.6745942831039429\n",
      "Iteration 5200/200000, loss=0.7128412127494812\n",
      "Iteration 5300/200000, loss=0.7015948295593262\n",
      "Iteration 5400/200000, loss=0.7033419609069824\n",
      "Iteration 5500/200000, loss=0.6936413049697876\n",
      "Iteration 5600/200000, loss=0.7013341784477234\n",
      "Iteration 5700/200000, loss=0.6893398761749268\n",
      "Iteration 5800/200000, loss=0.7043500542640686\n",
      "Iteration 5900/200000, loss=0.6665228605270386\n",
      "Iteration 6000/200000, loss=0.724665641784668\n",
      "Iteration 6100/200000, loss=0.7028259038925171\n",
      "Iteration 6200/200000, loss=0.7492660284042358\n",
      "Iteration 6300/200000, loss=0.7283622026443481\n",
      "Iteration 6400/200000, loss=0.7320479154586792\n",
      "Iteration 6500/200000, loss=0.7426153421401978\n",
      "Iteration 6600/200000, loss=0.7204774022102356\n",
      "Iteration 6700/200000, loss=0.6572814583778381\n",
      "Iteration 6800/200000, loss=0.7404725551605225\n",
      "Iteration 6900/200000, loss=0.6622868776321411\n",
      "Iteration 7000/200000, loss=0.6599103212356567\n",
      "Iteration 7100/200000, loss=0.7134603261947632\n",
      "Iteration 7200/200000, loss=0.681914210319519\n",
      "Iteration 7300/200000, loss=0.7423372268676758\n",
      "Iteration 7400/200000, loss=0.6680845022201538\n",
      "Iteration 7500/200000, loss=0.7358576655387878\n",
      "Iteration 7600/200000, loss=0.67354416847229\n",
      "Iteration 7700/200000, loss=0.7103556394577026\n",
      "Iteration 7800/200000, loss=0.68468177318573\n",
      "Iteration 7900/200000, loss=0.6871213316917419\n",
      "Iteration 8000/200000, loss=0.7096805572509766\n",
      "Iteration 8100/200000, loss=0.6749452352523804\n",
      "Iteration 8200/200000, loss=0.7028342485427856\n",
      "Iteration 8300/200000, loss=0.7136236429214478\n",
      "Iteration 8400/200000, loss=0.6684339046478271\n",
      "Iteration 8500/200000, loss=0.7084405422210693\n",
      "Iteration 8600/200000, loss=0.6786327362060547\n",
      "Iteration 8700/200000, loss=0.7003576159477234\n",
      "Iteration 8800/200000, loss=0.6643790602684021\n",
      "Iteration 8900/200000, loss=0.6936857104301453\n",
      "Iteration 9000/200000, loss=0.6728352308273315\n",
      "Iteration 9100/200000, loss=0.727839469909668\n",
      "Iteration 9200/200000, loss=0.6813040375709534\n",
      "Iteration 9300/200000, loss=0.6801065802574158\n",
      "Iteration 9400/200000, loss=0.6798391342163086\n",
      "Iteration 9500/200000, loss=0.7250673770904541\n",
      "Iteration 9600/200000, loss=0.6976497173309326\n",
      "Iteration 9700/200000, loss=0.7000432014465332\n",
      "Iteration 9800/200000, loss=0.6930238008499146\n",
      "Iteration 9900/200000, loss=0.6823035478591919\n",
      "Iteration 10000/200000, loss=0.7109561562538147\n",
      "Iteration 10100/200000, loss=0.6870614886283875\n",
      "Iteration 10200/200000, loss=0.6962990760803223\n",
      "Iteration 10300/200000, loss=0.6991313695907593\n",
      "Iteration 10400/200000, loss=0.7015052437782288\n",
      "Iteration 10500/200000, loss=0.664281964302063\n",
      "Iteration 10600/200000, loss=0.6684318780899048\n",
      "Iteration 10700/200000, loss=0.665001630783081\n",
      "Iteration 10800/200000, loss=0.6545761823654175\n",
      "Iteration 10900/200000, loss=0.7026432752609253\n",
      "Iteration 11000/200000, loss=0.7016607522964478\n",
      "Iteration 11100/200000, loss=0.6817868947982788\n",
      "Iteration 11200/200000, loss=0.6628727316856384\n",
      "Iteration 11300/200000, loss=0.7454649209976196\n",
      "Iteration 11400/200000, loss=0.7214047312736511\n",
      "Iteration 11500/200000, loss=0.6732412576675415\n",
      "Iteration 11600/200000, loss=0.7179279923439026\n",
      "Iteration 11700/200000, loss=0.6827038526535034\n",
      "Iteration 11800/200000, loss=0.7015924453735352\n",
      "Iteration 11900/200000, loss=0.6663398146629333\n",
      "Iteration 12000/200000, loss=0.6758502721786499\n",
      "Iteration 12100/200000, loss=0.6897749304771423\n",
      "Iteration 12200/200000, loss=0.6757392287254333\n",
      "Iteration 12300/200000, loss=0.7046223878860474\n",
      "Iteration 12400/200000, loss=0.7012410163879395\n",
      "Iteration 12500/200000, loss=0.7372744679450989\n",
      "Iteration 12600/200000, loss=0.7013663053512573\n",
      "Iteration 12700/200000, loss=0.6877553462982178\n",
      "Iteration 12800/200000, loss=0.710131049156189\n",
      "Iteration 12900/200000, loss=0.6923479437828064\n",
      "Iteration 13000/200000, loss=0.6988446116447449\n",
      "Iteration 13100/200000, loss=0.7166290879249573\n",
      "Iteration 13200/200000, loss=0.6748478412628174\n",
      "Iteration 13300/200000, loss=0.701737105846405\n",
      "Iteration 13400/200000, loss=0.6737504005432129\n",
      "Iteration 13500/200000, loss=0.6827341914176941\n",
      "Iteration 13600/200000, loss=0.6719141602516174\n",
      "Iteration 13700/200000, loss=0.6963924169540405\n",
      "Iteration 13800/200000, loss=0.7115839719772339\n",
      "Iteration 13900/200000, loss=0.6706960201263428\n",
      "Iteration 14000/200000, loss=0.6830005049705505\n",
      "Iteration 14100/200000, loss=0.700937807559967\n",
      "Iteration 14200/200000, loss=0.6702889800071716\n",
      "Iteration 14300/200000, loss=0.6661009192466736\n",
      "Iteration 14400/200000, loss=0.720947265625\n",
      "Iteration 14500/200000, loss=0.6655797958374023\n",
      "Iteration 14600/200000, loss=0.698631227016449\n",
      "Iteration 14700/200000, loss=0.6629088521003723\n",
      "Iteration 14800/200000, loss=0.6786753535270691\n",
      "Iteration 14900/200000, loss=0.7078770995140076\n",
      "Iteration 15000/200000, loss=0.6845546364784241\n",
      "Iteration 15100/200000, loss=0.7064056992530823\n",
      "Iteration 15200/200000, loss=0.7022650241851807\n",
      "Iteration 15300/200000, loss=0.6718167066574097\n",
      "Iteration 15400/200000, loss=0.7063630223274231\n",
      "Iteration 15500/200000, loss=0.6785721778869629\n",
      "Iteration 15600/200000, loss=0.6710847020149231\n",
      "Iteration 15700/200000, loss=0.6792230010032654\n",
      "Iteration 15800/200000, loss=0.6915602087974548\n",
      "Iteration 15900/200000, loss=0.7189892530441284\n",
      "Iteration 16000/200000, loss=0.6938128471374512\n",
      "Iteration 16100/200000, loss=0.6998788714408875\n",
      "Iteration 16200/200000, loss=0.6968778371810913\n",
      "Iteration 16300/200000, loss=0.6894966959953308\n",
      "Iteration 16400/200000, loss=0.7272219061851501\n",
      "Iteration 16500/200000, loss=0.694122314453125\n",
      "Iteration 16600/200000, loss=0.7293016314506531\n",
      "Iteration 16700/200000, loss=0.712370753288269\n",
      "Iteration 16800/200000, loss=0.691993772983551\n",
      "Iteration 16900/200000, loss=0.687140166759491\n",
      "Iteration 17000/200000, loss=0.665411651134491\n",
      "Iteration 17100/200000, loss=0.6841428279876709\n",
      "Iteration 17200/200000, loss=0.6856896877288818\n",
      "Iteration 17300/200000, loss=0.6799988150596619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17400/200000, loss=0.6838504672050476\n",
      "Iteration 17500/200000, loss=0.6946412324905396\n",
      "Iteration 17600/200000, loss=0.671798050403595\n",
      "Iteration 17700/200000, loss=0.6957928538322449\n",
      "Iteration 17800/200000, loss=0.6897964477539062\n",
      "Iteration 17900/200000, loss=0.7141585350036621\n",
      "Iteration 18000/200000, loss=0.6678675413131714\n",
      "Iteration 18100/200000, loss=0.7016836404800415\n",
      "Iteration 18200/200000, loss=0.6788962483406067\n",
      "Iteration 18300/200000, loss=0.6845566630363464\n",
      "Iteration 18400/200000, loss=0.7214685678482056\n",
      "Iteration 18500/200000, loss=0.6979836821556091\n",
      "Iteration 18600/200000, loss=0.7119431495666504\n",
      "Iteration 18700/200000, loss=0.6975154876708984\n",
      "Iteration 18800/200000, loss=0.6932817101478577\n",
      "Iteration 18900/200000, loss=0.7248334288597107\n",
      "Iteration 19000/200000, loss=0.6884815692901611\n",
      "Iteration 19100/200000, loss=0.676348865032196\n",
      "Iteration 19200/200000, loss=0.685945987701416\n",
      "Iteration 19300/200000, loss=0.6843099594116211\n",
      "Iteration 19400/200000, loss=0.7271061539649963\n",
      "Iteration 19500/200000, loss=0.675603449344635\n",
      "Iteration 19600/200000, loss=0.6743995547294617\n",
      "Iteration 19700/200000, loss=0.6846611499786377\n",
      "Iteration 19800/200000, loss=0.7030811309814453\n",
      "Iteration 19900/200000, loss=0.6834002733230591\n",
      "Iteration 20000/200000, loss=0.7079302072525024\n",
      "Iteration 20100/200000, loss=0.7218146920204163\n",
      "Iteration 20200/200000, loss=0.7226976156234741\n",
      "Iteration 20300/200000, loss=0.7163481712341309\n",
      "Iteration 20400/200000, loss=0.6499012112617493\n",
      "Iteration 20500/200000, loss=0.7406477928161621\n",
      "Iteration 20600/200000, loss=0.6625261902809143\n",
      "Iteration 20700/200000, loss=0.722068190574646\n",
      "Iteration 20800/200000, loss=0.7056199908256531\n",
      "Iteration 20900/200000, loss=0.7085027694702148\n",
      "Iteration 21000/200000, loss=0.682255208492279\n",
      "Iteration 21100/200000, loss=0.691156804561615\n",
      "Iteration 21200/200000, loss=0.66286301612854\n",
      "Iteration 21300/200000, loss=0.6544986963272095\n",
      "Iteration 21400/200000, loss=0.649721086025238\n",
      "Iteration 21500/200000, loss=0.7336825728416443\n",
      "Iteration 21600/200000, loss=0.7206176519393921\n",
      "Iteration 21700/200000, loss=0.6633977890014648\n",
      "Iteration 21800/200000, loss=0.7341420650482178\n",
      "Iteration 21900/200000, loss=0.710580050945282\n",
      "Iteration 22000/200000, loss=0.7287951707839966\n",
      "Iteration 22100/200000, loss=0.6635342836380005\n",
      "Iteration 22200/200000, loss=0.7215107679367065\n",
      "Iteration 22300/200000, loss=0.6549539566040039\n",
      "Iteration 22400/200000, loss=0.6708200573921204\n",
      "Iteration 22500/200000, loss=0.727705180644989\n",
      "Iteration 22600/200000, loss=0.661632776260376\n",
      "Iteration 22700/200000, loss=0.7446058392524719\n",
      "Iteration 22800/200000, loss=0.7181088328361511\n",
      "Iteration 22900/200000, loss=0.7060192823410034\n",
      "Iteration 23000/200000, loss=0.7054257392883301\n",
      "Iteration 23100/200000, loss=0.6873196959495544\n",
      "Iteration 23200/200000, loss=0.6991496682167053\n",
      "Iteration 23300/200000, loss=0.6976324915885925\n",
      "Iteration 23400/200000, loss=0.6853139400482178\n",
      "Iteration 23500/200000, loss=0.6779943704605103\n",
      "Iteration 23600/200000, loss=0.7049351930618286\n",
      "Iteration 23700/200000, loss=0.6927406191825867\n",
      "Iteration 23800/200000, loss=0.7100295424461365\n",
      "Iteration 23900/200000, loss=0.6596212387084961\n",
      "Iteration 24000/200000, loss=0.6743990778923035\n",
      "Iteration 24100/200000, loss=0.6893322467803955\n",
      "Iteration 24200/200000, loss=0.6875090599060059\n",
      "Iteration 24300/200000, loss=0.7236621379852295\n",
      "Iteration 24400/200000, loss=0.7289832234382629\n",
      "Iteration 24500/200000, loss=0.6945555210113525\n",
      "Iteration 24600/200000, loss=0.7168812155723572\n",
      "Iteration 24700/200000, loss=0.7308265566825867\n",
      "Iteration 24800/200000, loss=0.708844006061554\n",
      "Iteration 24900/200000, loss=0.7170450091362\n",
      "Iteration 25000/200000, loss=0.703456461429596\n",
      "Iteration 25100/200000, loss=0.6543066501617432\n",
      "Iteration 25200/200000, loss=0.7214601635932922\n",
      "Iteration 25300/200000, loss=0.7024396657943726\n",
      "Iteration 25400/200000, loss=0.6996488571166992\n",
      "Iteration 25500/200000, loss=0.676948606967926\n",
      "Iteration 25600/200000, loss=0.7282191514968872\n",
      "Iteration 25700/200000, loss=0.7016085982322693\n",
      "Iteration 25800/200000, loss=0.6967811584472656\n",
      "Iteration 25900/200000, loss=0.6833361387252808\n",
      "Iteration 26000/200000, loss=0.6850053668022156\n",
      "Iteration 26100/200000, loss=0.7056189775466919\n",
      "Iteration 26200/200000, loss=0.6796245574951172\n",
      "Iteration 26300/200000, loss=0.7188808917999268\n",
      "Iteration 26400/200000, loss=0.7251752614974976\n",
      "Iteration 26500/200000, loss=0.706335723400116\n",
      "Iteration 26600/200000, loss=0.6881621479988098\n",
      "Iteration 26700/200000, loss=0.7110169529914856\n",
      "Iteration 26800/200000, loss=0.7042744755744934\n",
      "Iteration 26900/200000, loss=0.6931784152984619\n",
      "Iteration 27000/200000, loss=0.6990985870361328\n",
      "Iteration 27100/200000, loss=0.6868494153022766\n",
      "Iteration 27200/200000, loss=0.6735048890113831\n",
      "Iteration 27300/200000, loss=0.6801167726516724\n",
      "Iteration 27400/200000, loss=0.6788821816444397\n",
      "Iteration 27500/200000, loss=0.6891937255859375\n",
      "Iteration 27600/200000, loss=0.6896398663520813\n",
      "Iteration 27700/200000, loss=0.6751987338066101\n",
      "Iteration 27800/200000, loss=0.6980254650115967\n",
      "Iteration 27900/200000, loss=0.6816622614860535\n",
      "Iteration 28000/200000, loss=0.6924271583557129\n",
      "Iteration 28100/200000, loss=0.6992169618606567\n",
      "Iteration 28200/200000, loss=0.6877427101135254\n",
      "Iteration 28300/200000, loss=0.6991539001464844\n",
      "Iteration 28400/200000, loss=0.7106543779373169\n",
      "Iteration 28500/200000, loss=0.7250134348869324\n",
      "Iteration 28600/200000, loss=0.6581839919090271\n",
      "Iteration 28700/200000, loss=0.6783300638198853\n",
      "Iteration 28800/200000, loss=0.6763975024223328\n",
      "Iteration 28900/200000, loss=0.716392993927002\n",
      "Iteration 29000/200000, loss=0.6917626857757568\n",
      "Iteration 29100/200000, loss=0.7157617807388306\n",
      "Iteration 29200/200000, loss=0.7091500163078308\n",
      "Iteration 29300/200000, loss=0.7133853435516357\n",
      "Iteration 29400/200000, loss=0.7234121561050415\n",
      "Iteration 29500/200000, loss=0.7133669257164001\n",
      "Iteration 29600/200000, loss=0.700494647026062\n",
      "Iteration 29700/200000, loss=0.6943726539611816\n",
      "Iteration 29800/200000, loss=0.6620073318481445\n",
      "Iteration 29900/200000, loss=0.6505439281463623\n",
      "Iteration 30000/200000, loss=0.6815651655197144\n",
      "Iteration 30100/200000, loss=0.6665956974029541\n",
      "Iteration 30200/200000, loss=0.6695842742919922\n",
      "Iteration 30300/200000, loss=0.73079514503479\n",
      "Iteration 30400/200000, loss=0.670807957649231\n",
      "Iteration 30500/200000, loss=0.7205774188041687\n",
      "Iteration 30600/200000, loss=0.6870754957199097\n",
      "Iteration 30700/200000, loss=0.7190380096435547\n",
      "Iteration 30800/200000, loss=0.6675832271575928\n",
      "Iteration 30900/200000, loss=0.699992299079895\n",
      "Iteration 31000/200000, loss=0.7122255563735962\n",
      "Iteration 31100/200000, loss=0.6712677478790283\n",
      "Iteration 31200/200000, loss=0.6995723247528076\n",
      "Iteration 31300/200000, loss=0.6854568123817444\n",
      "Iteration 31400/200000, loss=0.7144650220870972\n",
      "Iteration 31500/200000, loss=0.701224684715271\n",
      "Iteration 31600/200000, loss=0.7455325126647949\n",
      "Iteration 31700/200000, loss=0.72914057970047\n",
      "Iteration 31800/200000, loss=0.6825149059295654\n",
      "Iteration 31900/200000, loss=0.7028356790542603\n",
      "Iteration 32000/200000, loss=0.6588570475578308\n",
      "Iteration 32100/200000, loss=0.6766548156738281\n",
      "Iteration 32200/200000, loss=0.6995731592178345\n",
      "Iteration 32300/200000, loss=0.6926230788230896\n",
      "Iteration 32400/200000, loss=0.6837347149848938\n",
      "Iteration 32500/200000, loss=0.7068309783935547\n",
      "Iteration 32600/200000, loss=0.6875782012939453\n",
      "Iteration 32700/200000, loss=0.6853015422821045\n",
      "Iteration 32800/200000, loss=0.6863757371902466\n",
      "Iteration 32900/200000, loss=0.6748016476631165\n",
      "Iteration 33000/200000, loss=0.7171351909637451\n",
      "Iteration 33100/200000, loss=0.7406927347183228\n",
      "Iteration 33200/200000, loss=0.7065955996513367\n",
      "Iteration 33300/200000, loss=0.7016486525535583\n",
      "Iteration 33400/200000, loss=0.7011271119117737\n",
      "Iteration 33500/200000, loss=0.6832754611968994\n",
      "Iteration 33600/200000, loss=0.681328535079956\n",
      "Iteration 33700/200000, loss=0.6669148802757263\n",
      "Iteration 33800/200000, loss=0.6899603605270386\n",
      "Iteration 33900/200000, loss=0.6938092708587646\n",
      "Iteration 34000/200000, loss=0.7091779708862305\n",
      "Iteration 34100/200000, loss=0.6849132776260376\n",
      "Iteration 34200/200000, loss=0.6956997513771057\n",
      "Iteration 34300/200000, loss=0.7106251120567322\n",
      "Iteration 34400/200000, loss=0.6830625534057617\n",
      "Iteration 34500/200000, loss=0.679957926273346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34600/200000, loss=0.6865646839141846\n",
      "Iteration 34700/200000, loss=0.7012349963188171\n",
      "Iteration 34800/200000, loss=0.7029752135276794\n",
      "Iteration 34900/200000, loss=0.7191710472106934\n",
      "Iteration 35000/200000, loss=0.7136729955673218\n",
      "Iteration 35100/200000, loss=0.6925614476203918\n",
      "Iteration 35200/200000, loss=0.6935650110244751\n",
      "Iteration 35300/200000, loss=0.695820152759552\n",
      "Iteration 35400/200000, loss=0.6920179724693298\n",
      "Iteration 35500/200000, loss=0.7051374912261963\n",
      "Iteration 35600/200000, loss=0.695364773273468\n",
      "Iteration 35700/200000, loss=0.691085159778595\n",
      "Iteration 35800/200000, loss=0.6608214974403381\n",
      "Iteration 35900/200000, loss=0.722712516784668\n",
      "Iteration 36000/200000, loss=0.6997109055519104\n",
      "Iteration 36100/200000, loss=0.6697403192520142\n",
      "Iteration 36200/200000, loss=0.6992559432983398\n",
      "Iteration 36300/200000, loss=0.6882035136222839\n",
      "Iteration 36400/200000, loss=0.6776135563850403\n",
      "Iteration 36500/200000, loss=0.6870596408843994\n",
      "Iteration 36600/200000, loss=0.6903831958770752\n",
      "Iteration 36700/200000, loss=0.6722853183746338\n",
      "Iteration 36800/200000, loss=0.6803140044212341\n",
      "Iteration 36900/200000, loss=0.6945949792861938\n",
      "Iteration 37000/200000, loss=0.7002127170562744\n",
      "Iteration 37100/200000, loss=0.6988956332206726\n",
      "Iteration 37200/200000, loss=0.721210241317749\n",
      "Iteration 37300/200000, loss=0.7078468203544617\n",
      "Iteration 37400/200000, loss=0.6981664896011353\n",
      "Iteration 37500/200000, loss=0.669196605682373\n",
      "Iteration 37600/200000, loss=0.6953625082969666\n",
      "Iteration 37700/200000, loss=0.6930832862854004\n",
      "Iteration 37800/200000, loss=0.6856278777122498\n",
      "Iteration 37900/200000, loss=0.7070665955543518\n",
      "Iteration 38000/200000, loss=0.6781092882156372\n",
      "Iteration 38100/200000, loss=0.7013170719146729\n",
      "Iteration 38200/200000, loss=0.6943915486335754\n",
      "Iteration 38300/200000, loss=0.6899957060813904\n",
      "Iteration 38400/200000, loss=0.7063410878181458\n",
      "Iteration 38500/200000, loss=0.7069268226623535\n",
      "Iteration 38600/200000, loss=0.7084490060806274\n",
      "Iteration 38700/200000, loss=0.6881693601608276\n",
      "Iteration 38800/200000, loss=0.6949875354766846\n",
      "Iteration 38900/200000, loss=0.6923290491104126\n",
      "Iteration 39000/200000, loss=0.6829466819763184\n",
      "Iteration 39100/200000, loss=0.7151398658752441\n",
      "Iteration 39200/200000, loss=0.6862892508506775\n",
      "Iteration 39300/200000, loss=0.6950463056564331\n",
      "Iteration 39400/200000, loss=0.6865277290344238\n",
      "Iteration 39500/200000, loss=0.7015442848205566\n",
      "Iteration 39600/200000, loss=0.6657967567443848\n",
      "Iteration 39700/200000, loss=0.7146843671798706\n",
      "Iteration 39800/200000, loss=0.693732738494873\n",
      "Iteration 39900/200000, loss=0.6700372695922852\n",
      "Iteration 40000/200000, loss=0.6824458241462708\n",
      "Iteration 40100/200000, loss=0.6454648375511169\n",
      "Iteration 40200/200000, loss=0.7339440584182739\n",
      "Iteration 40300/200000, loss=0.7174080014228821\n",
      "Iteration 40400/200000, loss=0.6990503072738647\n",
      "Iteration 40500/200000, loss=0.6771030426025391\n",
      "Iteration 40600/200000, loss=0.713333785533905\n",
      "Iteration 40700/200000, loss=0.7197964191436768\n",
      "Iteration 40800/200000, loss=0.6812034249305725\n",
      "Iteration 40900/200000, loss=0.6797083616256714\n",
      "Iteration 41000/200000, loss=0.6842302680015564\n",
      "Iteration 41100/200000, loss=0.6966274976730347\n",
      "Iteration 41200/200000, loss=0.6783546805381775\n",
      "Iteration 41300/200000, loss=0.6812635064125061\n",
      "Iteration 41400/200000, loss=0.7074118852615356\n",
      "Iteration 41500/200000, loss=0.694560706615448\n",
      "Iteration 41600/200000, loss=0.6850031018257141\n",
      "Iteration 41700/200000, loss=0.6880452036857605\n",
      "Iteration 41800/200000, loss=0.6983020305633545\n",
      "Iteration 41900/200000, loss=0.7092820405960083\n",
      "Iteration 42000/200000, loss=0.6714450716972351\n",
      "Iteration 42100/200000, loss=0.676568865776062\n",
      "Iteration 42200/200000, loss=0.6823571920394897\n",
      "Iteration 42300/200000, loss=0.6673641204833984\n",
      "Iteration 42400/200000, loss=0.7037681937217712\n",
      "Iteration 42500/200000, loss=0.7163214087486267\n",
      "Iteration 42600/200000, loss=0.7315748929977417\n",
      "Iteration 42700/200000, loss=0.6755179166793823\n",
      "Iteration 42800/200000, loss=0.7166767120361328\n",
      "Iteration 42900/200000, loss=0.6968026757240295\n",
      "Iteration 43000/200000, loss=0.710369348526001\n",
      "Iteration 43100/200000, loss=0.745037853717804\n",
      "Iteration 43200/200000, loss=0.722385585308075\n",
      "Iteration 43300/200000, loss=0.6562382578849792\n",
      "Iteration 43400/200000, loss=0.7164793610572815\n",
      "Iteration 43500/200000, loss=0.6737459897994995\n",
      "Iteration 43600/200000, loss=0.6644740104675293\n",
      "Iteration 43700/200000, loss=0.687125563621521\n",
      "Iteration 43800/200000, loss=0.7100889682769775\n",
      "Iteration 43900/200000, loss=0.7199686765670776\n",
      "Iteration 44000/200000, loss=0.6785368919372559\n",
      "Iteration 44100/200000, loss=0.7012128233909607\n",
      "Iteration 44200/200000, loss=0.7025402784347534\n",
      "Iteration 44300/200000, loss=0.6645412445068359\n",
      "Iteration 44400/200000, loss=0.6859588027000427\n",
      "Iteration 44500/200000, loss=0.7051541209220886\n",
      "Iteration 44600/200000, loss=0.6678292155265808\n",
      "Iteration 44700/200000, loss=0.6778638362884521\n",
      "Iteration 44800/200000, loss=0.7122936248779297\n",
      "Iteration 44900/200000, loss=0.6955533623695374\n",
      "Iteration 45000/200000, loss=0.6907795071601868\n",
      "Iteration 45100/200000, loss=0.6826849579811096\n",
      "Iteration 45200/200000, loss=0.6869099736213684\n",
      "Iteration 45300/200000, loss=0.6736245155334473\n",
      "Iteration 45400/200000, loss=0.7064708471298218\n",
      "Iteration 45500/200000, loss=0.6876761317253113\n",
      "Iteration 45600/200000, loss=0.7059723138809204\n",
      "Iteration 45700/200000, loss=0.7112683057785034\n",
      "Iteration 45800/200000, loss=0.709998369216919\n",
      "Iteration 45900/200000, loss=0.7057201862335205\n",
      "Iteration 46000/200000, loss=0.6795253157615662\n",
      "Iteration 46100/200000, loss=0.6846712827682495\n",
      "Iteration 46200/200000, loss=0.6833198070526123\n",
      "Iteration 46300/200000, loss=0.7199710607528687\n",
      "Iteration 46400/200000, loss=0.6623567938804626\n",
      "Iteration 46500/200000, loss=0.6640195250511169\n",
      "Iteration 46600/200000, loss=0.7062585353851318\n",
      "Iteration 46700/200000, loss=0.6909381747245789\n",
      "Iteration 46800/200000, loss=0.6925875544548035\n",
      "Iteration 46900/200000, loss=0.670031726360321\n",
      "Iteration 47000/200000, loss=0.7374729514122009\n",
      "Iteration 47100/200000, loss=0.7170513272285461\n",
      "Iteration 47200/200000, loss=0.65755695104599\n",
      "Iteration 47300/200000, loss=0.689142644405365\n",
      "Iteration 47400/200000, loss=0.6699624061584473\n",
      "Iteration 47500/200000, loss=0.6751775741577148\n",
      "Iteration 47600/200000, loss=0.6910793781280518\n",
      "Iteration 47700/200000, loss=0.6294378638267517\n",
      "Iteration 47800/200000, loss=0.6346517205238342\n",
      "Iteration 47900/200000, loss=0.7447955012321472\n",
      "Iteration 48000/200000, loss=0.7543684244155884\n",
      "Iteration 48100/200000, loss=0.6538383364677429\n",
      "Iteration 48200/200000, loss=0.7254474759101868\n",
      "Iteration 48300/200000, loss=0.6816598176956177\n",
      "Iteration 48400/200000, loss=0.7147879600524902\n",
      "Iteration 48500/200000, loss=0.653817355632782\n",
      "Iteration 48600/200000, loss=0.7254820466041565\n",
      "Iteration 48700/200000, loss=0.7288433313369751\n",
      "Iteration 48800/200000, loss=0.7164235711097717\n",
      "Iteration 48900/200000, loss=0.7167478799819946\n",
      "Iteration 49000/200000, loss=0.7186141014099121\n",
      "Iteration 49100/200000, loss=0.7085950374603271\n",
      "Iteration 49200/200000, loss=0.7201719284057617\n",
      "Iteration 49300/200000, loss=0.7273081541061401\n",
      "Iteration 49400/200000, loss=0.6982533931732178\n",
      "Iteration 49500/200000, loss=0.7148418426513672\n",
      "Iteration 49600/200000, loss=0.7026275396347046\n",
      "Iteration 49700/200000, loss=0.6754503846168518\n",
      "Iteration 49800/200000, loss=0.6625727415084839\n",
      "Iteration 49900/200000, loss=0.7107453346252441\n",
      "Iteration 50000/200000, loss=0.6965407729148865\n",
      "Iteration 50100/200000, loss=0.6810630559921265\n",
      "Iteration 50200/200000, loss=0.6800918579101562\n",
      "Iteration 50300/200000, loss=0.6629025340080261\n",
      "Iteration 50400/200000, loss=0.6859409809112549\n",
      "Iteration 50500/200000, loss=0.6748458743095398\n",
      "Iteration 50600/200000, loss=0.6989818215370178\n",
      "Iteration 50700/200000, loss=0.7119647264480591\n",
      "Iteration 50800/200000, loss=0.6961941123008728\n",
      "Iteration 50900/200000, loss=0.7024362087249756\n",
      "Iteration 51000/200000, loss=0.6543164253234863\n",
      "Iteration 51100/200000, loss=0.6855001449584961\n",
      "Iteration 51200/200000, loss=0.7277913093566895\n",
      "Iteration 51300/200000, loss=0.6955574750900269\n",
      "Iteration 51400/200000, loss=0.6966463327407837\n",
      "Iteration 51500/200000, loss=0.7178491950035095\n",
      "Iteration 51600/200000, loss=0.7042914032936096\n",
      "Iteration 51700/200000, loss=0.6934952139854431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 51800/200000, loss=0.6777366399765015\n",
      "Iteration 51900/200000, loss=0.6840250492095947\n",
      "Iteration 52000/200000, loss=0.7137592434883118\n",
      "Iteration 52100/200000, loss=0.7081481218338013\n",
      "Iteration 52200/200000, loss=0.7054582834243774\n",
      "Iteration 52300/200000, loss=0.6537220478057861\n",
      "Iteration 52400/200000, loss=0.6871859431266785\n",
      "Iteration 52500/200000, loss=0.6987343430519104\n",
      "Iteration 52600/200000, loss=0.6859804391860962\n",
      "Iteration 52700/200000, loss=0.6979339122772217\n",
      "Iteration 52800/200000, loss=0.7159368991851807\n",
      "Iteration 52900/200000, loss=0.689119279384613\n",
      "Iteration 53000/200000, loss=0.7090778946876526\n",
      "Iteration 53100/200000, loss=0.7101702094078064\n",
      "Iteration 53200/200000, loss=0.6713685989379883\n",
      "Iteration 53300/200000, loss=0.6981118321418762\n",
      "Iteration 53400/200000, loss=0.6812922358512878\n",
      "Iteration 53500/200000, loss=0.6781622767448425\n",
      "Iteration 53600/200000, loss=0.7165370583534241\n",
      "Iteration 53700/200000, loss=0.7037896513938904\n",
      "Iteration 53800/200000, loss=0.6742451190948486\n",
      "Iteration 53900/200000, loss=0.6751798987388611\n",
      "Iteration 54000/200000, loss=0.7156983613967896\n",
      "Iteration 54100/200000, loss=0.689411461353302\n",
      "Iteration 54200/200000, loss=0.7350558042526245\n",
      "Iteration 54300/200000, loss=0.7008565068244934\n",
      "Iteration 54400/200000, loss=0.6870408058166504\n",
      "Iteration 54500/200000, loss=0.6937112212181091\n",
      "Iteration 54600/200000, loss=0.6699667572975159\n",
      "Iteration 54700/200000, loss=0.7204601764678955\n",
      "Iteration 54800/200000, loss=0.7027131915092468\n",
      "Iteration 54900/200000, loss=0.6738013029098511\n",
      "Iteration 55000/200000, loss=0.6787317991256714\n",
      "Iteration 55100/200000, loss=0.709491491317749\n",
      "Iteration 55200/200000, loss=0.6918891072273254\n",
      "Iteration 55300/200000, loss=0.6895614862442017\n",
      "Iteration 55400/200000, loss=0.6943249106407166\n",
      "Iteration 55500/200000, loss=0.6977824568748474\n",
      "Iteration 55600/200000, loss=0.7142155766487122\n",
      "Iteration 55700/200000, loss=0.71712327003479\n",
      "Iteration 55800/200000, loss=0.6549302935600281\n",
      "Iteration 55900/200000, loss=0.7342442274093628\n",
      "Iteration 56000/200000, loss=0.7016525864601135\n",
      "Iteration 56100/200000, loss=0.7116787433624268\n",
      "Iteration 56200/200000, loss=0.7397847771644592\n",
      "Iteration 56300/200000, loss=0.7051903009414673\n",
      "Iteration 56400/200000, loss=0.7131620645523071\n",
      "Iteration 56500/200000, loss=0.6663141250610352\n",
      "Iteration 56600/200000, loss=0.64415043592453\n",
      "Iteration 56700/200000, loss=0.6764811873435974\n",
      "Iteration 56800/200000, loss=0.7405023574829102\n",
      "Iteration 56900/200000, loss=0.693753182888031\n",
      "Iteration 57000/200000, loss=0.6615214943885803\n",
      "Iteration 57100/200000, loss=0.6858282089233398\n",
      "Iteration 57200/200000, loss=0.6846026182174683\n",
      "Iteration 57300/200000, loss=0.6744441390037537\n",
      "Iteration 57400/200000, loss=0.6823886036872864\n",
      "Iteration 57500/200000, loss=0.6557854413986206\n",
      "Iteration 57600/200000, loss=0.7042191028594971\n",
      "Iteration 57700/200000, loss=0.6881094574928284\n",
      "Iteration 57800/200000, loss=0.6863886713981628\n",
      "Iteration 57900/200000, loss=0.7063484191894531\n",
      "Iteration 58000/200000, loss=0.6899101734161377\n",
      "Iteration 58100/200000, loss=0.6809239387512207\n",
      "Iteration 58200/200000, loss=0.7000845670700073\n",
      "Iteration 58300/200000, loss=0.7050300240516663\n",
      "Iteration 58400/200000, loss=0.687528669834137\n",
      "Iteration 58500/200000, loss=0.74094557762146\n",
      "Iteration 58600/200000, loss=0.6758113503456116\n",
      "Iteration 58700/200000, loss=0.7302278876304626\n",
      "Iteration 58800/200000, loss=0.6911572813987732\n",
      "Iteration 58900/200000, loss=0.6877837777137756\n",
      "Iteration 59000/200000, loss=0.6784377098083496\n",
      "Iteration 59100/200000, loss=0.7170735597610474\n",
      "Iteration 59200/200000, loss=0.7117429375648499\n",
      "Iteration 59300/200000, loss=0.6850938200950623\n",
      "Iteration 59400/200000, loss=0.683708906173706\n",
      "Iteration 59500/200000, loss=0.6773972511291504\n",
      "Iteration 59600/200000, loss=0.651845395565033\n",
      "Iteration 59700/200000, loss=0.674772322177887\n",
      "Iteration 59800/200000, loss=0.6849456429481506\n",
      "Iteration 59900/200000, loss=0.7059565782546997\n",
      "Iteration 60000/200000, loss=0.6966378092765808\n",
      "Iteration 60100/200000, loss=0.6550678610801697\n",
      "Iteration 60200/200000, loss=0.6978344917297363\n",
      "Iteration 60300/200000, loss=0.7310818433761597\n",
      "Iteration 60400/200000, loss=0.6945465207099915\n",
      "Iteration 60500/200000, loss=0.6916830539703369\n",
      "Iteration 60600/200000, loss=0.6916253566741943\n",
      "Iteration 60700/200000, loss=0.6761260032653809\n",
      "Iteration 60800/200000, loss=0.71394282579422\n",
      "Iteration 60900/200000, loss=0.7186771035194397\n",
      "Iteration 61000/200000, loss=0.7397155165672302\n",
      "Iteration 61100/200000, loss=0.6710256338119507\n",
      "Iteration 61200/200000, loss=0.6584494113922119\n",
      "Iteration 61300/200000, loss=0.6674250364303589\n",
      "Iteration 61400/200000, loss=0.6810457110404968\n",
      "Iteration 61500/200000, loss=0.7211747765541077\n",
      "Iteration 61600/200000, loss=0.6783949136734009\n",
      "Iteration 61700/200000, loss=0.6754581928253174\n",
      "Iteration 61800/200000, loss=0.7146324515342712\n",
      "Iteration 61900/200000, loss=0.708099365234375\n",
      "Iteration 62000/200000, loss=0.7080878615379333\n",
      "Iteration 62100/200000, loss=0.6823381185531616\n",
      "Iteration 62200/200000, loss=0.6931326985359192\n",
      "Iteration 62300/200000, loss=0.6857390999794006\n",
      "Iteration 62400/200000, loss=0.6867058873176575\n",
      "Iteration 62500/200000, loss=0.7021927237510681\n",
      "Iteration 62600/200000, loss=0.7146306037902832\n",
      "Iteration 62700/200000, loss=0.7016868591308594\n",
      "Iteration 62800/200000, loss=0.6727750897407532\n",
      "Iteration 62900/200000, loss=0.6988853812217712\n",
      "Iteration 63000/200000, loss=0.6743339896202087\n",
      "Iteration 63100/200000, loss=0.7152860760688782\n",
      "Iteration 63200/200000, loss=0.7017039060592651\n",
      "Iteration 63300/200000, loss=0.6714538335800171\n",
      "Iteration 63400/200000, loss=0.6800944209098816\n",
      "Iteration 63500/200000, loss=0.7284975051879883\n",
      "Iteration 63600/200000, loss=0.7027738690376282\n",
      "Iteration 63700/200000, loss=0.6808302402496338\n",
      "Iteration 63800/200000, loss=0.6991587281227112\n",
      "Iteration 63900/200000, loss=0.7227755188941956\n",
      "Iteration 64000/200000, loss=0.6699031591415405\n",
      "Iteration 64100/200000, loss=0.6793625950813293\n",
      "Iteration 64200/200000, loss=0.6776652932167053\n",
      "Iteration 64300/200000, loss=0.715097963809967\n",
      "Iteration 64400/200000, loss=0.7019615769386292\n",
      "Iteration 64500/200000, loss=0.7100478410720825\n",
      "Iteration 64600/200000, loss=0.6731321215629578\n",
      "Iteration 64700/200000, loss=0.6889715194702148\n",
      "Iteration 64800/200000, loss=0.7052952647209167\n",
      "Iteration 64900/200000, loss=0.6799529194831848\n",
      "Iteration 65000/200000, loss=0.6886754035949707\n",
      "Iteration 65100/200000, loss=0.693969190120697\n",
      "Iteration 65200/200000, loss=0.6784930229187012\n",
      "Iteration 65300/200000, loss=0.6999630928039551\n",
      "Iteration 65400/200000, loss=0.7045761942863464\n",
      "Iteration 65500/200000, loss=0.7121806144714355\n",
      "Iteration 65600/200000, loss=0.7245659232139587\n",
      "Iteration 65700/200000, loss=0.6585242748260498\n",
      "Iteration 65800/200000, loss=0.6970618367195129\n",
      "Iteration 65900/200000, loss=0.6937131881713867\n",
      "Iteration 66000/200000, loss=0.6822729110717773\n",
      "Iteration 66100/200000, loss=0.6824613809585571\n",
      "Iteration 66200/200000, loss=0.6573904752731323\n",
      "Iteration 66300/200000, loss=0.6720755696296692\n",
      "Iteration 66400/200000, loss=0.6906643509864807\n",
      "Iteration 66500/200000, loss=0.7105719447135925\n",
      "Iteration 66600/200000, loss=0.7035636305809021\n",
      "Iteration 66700/200000, loss=0.703584611415863\n",
      "Iteration 66800/200000, loss=0.7227789163589478\n",
      "Iteration 66900/200000, loss=0.7105737328529358\n",
      "Iteration 67000/200000, loss=0.7288482785224915\n",
      "Iteration 67100/200000, loss=0.6824319958686829\n",
      "Iteration 67200/200000, loss=0.6785442233085632\n",
      "Iteration 67300/200000, loss=0.6413798928260803\n",
      "Iteration 67400/200000, loss=0.6864256858825684\n",
      "Iteration 67500/200000, loss=0.6902919411659241\n",
      "Iteration 67600/200000, loss=0.6713565587997437\n",
      "Iteration 67700/200000, loss=0.7024171352386475\n",
      "Iteration 67800/200000, loss=0.6993508338928223\n",
      "Iteration 67900/200000, loss=0.7063395380973816\n",
      "Iteration 68000/200000, loss=0.7246184349060059\n",
      "Iteration 68100/200000, loss=0.697090744972229\n",
      "Iteration 68200/200000, loss=0.72977614402771\n",
      "Iteration 68300/200000, loss=0.6824701428413391\n",
      "Iteration 68400/200000, loss=0.7146276235580444\n",
      "Iteration 68500/200000, loss=0.6480174660682678\n",
      "Iteration 68600/200000, loss=0.6977719068527222\n",
      "Iteration 68700/200000, loss=0.7075948715209961\n",
      "Iteration 68800/200000, loss=0.6980159282684326\n",
      "Iteration 68900/200000, loss=0.6910021305084229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 69000/200000, loss=0.7139599323272705\n",
      "Iteration 69100/200000, loss=0.6645857095718384\n",
      "Iteration 69200/200000, loss=0.7001447677612305\n",
      "Iteration 69300/200000, loss=0.6736506223678589\n",
      "Iteration 69400/200000, loss=0.702328622341156\n",
      "Iteration 69500/200000, loss=0.682843029499054\n",
      "Iteration 69600/200000, loss=0.6751419305801392\n",
      "Iteration 69700/200000, loss=0.7435110807418823\n",
      "Iteration 69800/200000, loss=0.6957598328590393\n",
      "Iteration 69900/200000, loss=0.6541163325309753\n",
      "Iteration 70000/200000, loss=0.7155376076698303\n",
      "Iteration 70100/200000, loss=0.681094229221344\n",
      "Iteration 70200/200000, loss=0.6943890452384949\n",
      "Iteration 70300/200000, loss=0.7025951147079468\n",
      "Iteration 70400/200000, loss=0.7010681629180908\n",
      "Iteration 70500/200000, loss=0.7180209159851074\n",
      "Iteration 70600/200000, loss=0.6770004630088806\n",
      "Iteration 70700/200000, loss=0.6904580593109131\n",
      "Iteration 70800/200000, loss=0.653156042098999\n",
      "Iteration 70900/200000, loss=0.7262290716171265\n",
      "Iteration 71000/200000, loss=0.6950562000274658\n",
      "Iteration 71100/200000, loss=0.7213222980499268\n",
      "Iteration 71200/200000, loss=0.6756704449653625\n",
      "Iteration 71300/200000, loss=0.6915526986122131\n",
      "Iteration 71400/200000, loss=0.6920559406280518\n",
      "Iteration 71500/200000, loss=0.6900779604911804\n",
      "Iteration 71600/200000, loss=0.6890155673027039\n",
      "Iteration 71700/200000, loss=0.6806780099868774\n",
      "Iteration 71800/200000, loss=0.6440486311912537\n",
      "Iteration 71900/200000, loss=0.7176116704940796\n",
      "Iteration 72000/200000, loss=0.6906608939170837\n",
      "Iteration 72100/200000, loss=0.6906974911689758\n",
      "Iteration 72200/200000, loss=0.6933683156967163\n",
      "Iteration 72300/200000, loss=0.6917441487312317\n",
      "Iteration 72400/200000, loss=0.7086451649665833\n",
      "Iteration 72500/200000, loss=0.7055507302284241\n",
      "Iteration 72600/200000, loss=0.7041635513305664\n",
      "Iteration 72700/200000, loss=0.7292134165763855\n",
      "Iteration 72800/200000, loss=0.7031319737434387\n",
      "Iteration 72900/200000, loss=0.6910545229911804\n",
      "Iteration 73000/200000, loss=0.6791766285896301\n",
      "Iteration 73100/200000, loss=0.7418094873428345\n",
      "Iteration 73200/200000, loss=0.667046308517456\n",
      "Iteration 73300/200000, loss=0.6850544810295105\n",
      "Iteration 73400/200000, loss=0.6774734854698181\n",
      "Iteration 73500/200000, loss=0.6748255491256714\n",
      "Iteration 73600/200000, loss=0.714862048625946\n",
      "Iteration 73700/200000, loss=0.702808141708374\n",
      "Iteration 73800/200000, loss=0.6973313689231873\n",
      "Iteration 73900/200000, loss=0.7241989970207214\n",
      "Iteration 74000/200000, loss=0.7196024656295776\n",
      "Iteration 74100/200000, loss=0.689272403717041\n",
      "Iteration 74200/200000, loss=0.6902440190315247\n",
      "Iteration 74300/200000, loss=0.6858443021774292\n",
      "Iteration 74400/200000, loss=0.6923399567604065\n",
      "Iteration 74500/200000, loss=0.7352912425994873\n",
      "Iteration 74600/200000, loss=0.7222374677658081\n",
      "Iteration 74700/200000, loss=0.6991598010063171\n",
      "Iteration 74800/200000, loss=0.7120800018310547\n",
      "Iteration 74900/200000, loss=0.7255194187164307\n",
      "Iteration 75000/200000, loss=0.6720989942550659\n",
      "Iteration 75100/200000, loss=0.6787022948265076\n",
      "Iteration 75200/200000, loss=0.667582631111145\n",
      "Iteration 75300/200000, loss=0.7020507454872131\n",
      "Iteration 75400/200000, loss=0.7109169363975525\n",
      "Iteration 75500/200000, loss=0.6931216716766357\n",
      "Iteration 75600/200000, loss=0.7099326848983765\n",
      "Iteration 75700/200000, loss=0.6962990760803223\n",
      "Iteration 75800/200000, loss=0.6900740265846252\n",
      "Iteration 75900/200000, loss=0.7250983119010925\n",
      "Iteration 76000/200000, loss=0.7006190419197083\n",
      "Iteration 76100/200000, loss=0.6776253581047058\n",
      "Iteration 76200/200000, loss=0.6709083914756775\n",
      "Iteration 76300/200000, loss=0.6646031141281128\n",
      "Iteration 76400/200000, loss=0.6465138792991638\n",
      "Iteration 76500/200000, loss=0.6890610456466675\n",
      "Iteration 76600/200000, loss=0.742206871509552\n",
      "Iteration 76700/200000, loss=0.6997298002243042\n",
      "Iteration 76800/200000, loss=0.6784417033195496\n",
      "Iteration 76900/200000, loss=0.6880494356155396\n",
      "Iteration 77000/200000, loss=0.7034610509872437\n",
      "Iteration 77100/200000, loss=0.682593047618866\n",
      "Iteration 77200/200000, loss=0.6872546672821045\n",
      "Iteration 77300/200000, loss=0.6706408858299255\n",
      "Iteration 77400/200000, loss=0.6936901807785034\n",
      "Iteration 77500/200000, loss=0.7010986804962158\n",
      "Iteration 77600/200000, loss=0.6671293377876282\n",
      "Iteration 77700/200000, loss=0.6797156929969788\n",
      "Iteration 77800/200000, loss=0.6891332268714905\n",
      "Iteration 77900/200000, loss=0.6907132267951965\n",
      "Iteration 78000/200000, loss=0.6763573288917542\n",
      "Iteration 78100/200000, loss=0.6893834471702576\n",
      "Iteration 78200/200000, loss=0.6980838179588318\n",
      "Iteration 78300/200000, loss=0.7096384167671204\n",
      "Iteration 78400/200000, loss=0.7325617074966431\n",
      "Iteration 78500/200000, loss=0.6549575924873352\n",
      "Iteration 78600/200000, loss=0.6589720845222473\n",
      "Iteration 78700/200000, loss=0.6843405961990356\n",
      "Iteration 78800/200000, loss=0.6992836594581604\n",
      "Iteration 78900/200000, loss=0.5988112688064575\n",
      "Iteration 79000/200000, loss=0.6691510081291199\n",
      "Iteration 79100/200000, loss=0.7006686329841614\n",
      "Iteration 79200/200000, loss=0.7101933360099792\n",
      "Iteration 79300/200000, loss=0.6844703555107117\n",
      "Iteration 79400/200000, loss=0.7047083377838135\n",
      "Iteration 79500/200000, loss=0.7304723262786865\n",
      "Iteration 79600/200000, loss=0.6915107369422913\n",
      "Iteration 79700/200000, loss=0.6986105442047119\n",
      "Iteration 79800/200000, loss=0.6888404488563538\n",
      "Iteration 79900/200000, loss=0.7050755620002747\n",
      "Iteration 80000/200000, loss=0.7247701287269592\n",
      "Iteration 80100/200000, loss=0.706985354423523\n",
      "Iteration 80200/200000, loss=0.6877629160881042\n",
      "Iteration 80300/200000, loss=0.6575169563293457\n",
      "Iteration 80400/200000, loss=0.6133722066879272\n",
      "Iteration 80500/200000, loss=0.7306330800056458\n",
      "Iteration 80600/200000, loss=0.6694353818893433\n",
      "Iteration 80700/200000, loss=0.6589818000793457\n",
      "Iteration 80800/200000, loss=0.7611024975776672\n",
      "Iteration 80900/200000, loss=0.6935530304908752\n",
      "Iteration 81000/200000, loss=0.7000755667686462\n",
      "Iteration 81100/200000, loss=0.7063170671463013\n",
      "Iteration 81200/200000, loss=0.6695379614830017\n",
      "Iteration 81300/200000, loss=0.685293436050415\n",
      "Iteration 81400/200000, loss=0.6981968879699707\n",
      "Iteration 81500/200000, loss=0.6877694725990295\n",
      "Iteration 81600/200000, loss=0.7204325795173645\n",
      "Iteration 81700/200000, loss=0.6968626379966736\n",
      "Iteration 81800/200000, loss=0.6600083112716675\n",
      "Iteration 81900/200000, loss=0.6923555731773376\n",
      "Iteration 82000/200000, loss=0.6990288496017456\n",
      "Iteration 82100/200000, loss=0.6684065461158752\n",
      "Iteration 82200/200000, loss=0.7005069851875305\n",
      "Iteration 82300/200000, loss=0.6550627946853638\n",
      "Iteration 82400/200000, loss=0.6716372966766357\n",
      "Iteration 82500/200000, loss=0.6937870979309082\n",
      "Iteration 82600/200000, loss=0.6843528151512146\n",
      "Iteration 82700/200000, loss=0.6820844411849976\n",
      "Iteration 82800/200000, loss=0.6724138259887695\n",
      "Iteration 82900/200000, loss=0.655142068862915\n",
      "Iteration 83000/200000, loss=0.7062587738037109\n",
      "Iteration 83100/200000, loss=0.689394474029541\n",
      "Iteration 83200/200000, loss=0.6642305850982666\n",
      "Iteration 83300/200000, loss=0.6823593974113464\n",
      "Iteration 83400/200000, loss=0.6705131530761719\n",
      "Iteration 83500/200000, loss=0.709754228591919\n",
      "Iteration 83600/200000, loss=0.6795141100883484\n",
      "Iteration 83700/200000, loss=0.6652898192405701\n",
      "Iteration 83800/200000, loss=0.6349701881408691\n",
      "Iteration 83900/200000, loss=0.6904765367507935\n",
      "Iteration 84000/200000, loss=0.7280163764953613\n",
      "Iteration 84100/200000, loss=0.686101496219635\n",
      "Iteration 84200/200000, loss=0.6469202637672424\n",
      "Iteration 84300/200000, loss=0.6918911337852478\n",
      "Iteration 84400/200000, loss=0.6816696524620056\n",
      "Iteration 84500/200000, loss=0.6925775408744812\n",
      "Iteration 84600/200000, loss=0.6721001863479614\n",
      "Iteration 84700/200000, loss=0.682342529296875\n",
      "Iteration 84800/200000, loss=0.6852118968963623\n",
      "Iteration 84900/200000, loss=0.7002021670341492\n",
      "Iteration 85000/200000, loss=0.7169106602668762\n",
      "Iteration 85100/200000, loss=0.689117431640625\n",
      "Iteration 85200/200000, loss=0.7126801013946533\n",
      "Iteration 85300/200000, loss=0.6938283443450928\n",
      "Iteration 85400/200000, loss=0.663175642490387\n",
      "Iteration 85500/200000, loss=0.6897789239883423\n",
      "Iteration 85600/200000, loss=0.6658132672309875\n",
      "Iteration 85700/200000, loss=0.7166270017623901\n",
      "Iteration 85800/200000, loss=0.7058312296867371\n",
      "Iteration 85900/200000, loss=0.6898476481437683\n",
      "Iteration 86000/200000, loss=0.683254063129425\n",
      "Iteration 86100/200000, loss=0.6934718489646912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 86200/200000, loss=0.7359734773635864\n",
      "Iteration 86300/200000, loss=0.6920121312141418\n",
      "Iteration 86400/200000, loss=0.6633764505386353\n",
      "Iteration 86500/200000, loss=0.7222808003425598\n",
      "Iteration 86600/200000, loss=0.6875405311584473\n",
      "Iteration 86700/200000, loss=0.6930172443389893\n",
      "Iteration 86800/200000, loss=0.6982826590538025\n",
      "Iteration 86900/200000, loss=0.6548715829849243\n",
      "Iteration 87000/200000, loss=0.7381437420845032\n",
      "Iteration 87100/200000, loss=0.6938593983650208\n",
      "Iteration 87200/200000, loss=0.6806643009185791\n",
      "Iteration 87300/200000, loss=0.7035122513771057\n",
      "Iteration 87400/200000, loss=0.7219244837760925\n",
      "Iteration 87500/200000, loss=0.748417317867279\n",
      "Iteration 87600/200000, loss=0.7128223776817322\n",
      "Iteration 87700/200000, loss=0.6968430280685425\n",
      "Iteration 87800/200000, loss=0.6709698438644409\n",
      "Iteration 87900/200000, loss=0.690081000328064\n",
      "Iteration 88000/200000, loss=0.7390635013580322\n",
      "Iteration 88100/200000, loss=0.6799781918525696\n",
      "Iteration 88200/200000, loss=0.7109104990959167\n",
      "Iteration 88300/200000, loss=0.6739274263381958\n",
      "Iteration 88400/200000, loss=0.7314350605010986\n",
      "Iteration 88500/200000, loss=0.6707713007926941\n",
      "Iteration 88600/200000, loss=0.6747475862503052\n",
      "Iteration 88700/200000, loss=0.6932555437088013\n",
      "Iteration 88800/200000, loss=0.6821561455726624\n",
      "Iteration 88900/200000, loss=0.6569429039955139\n",
      "Iteration 89000/200000, loss=0.7131155133247375\n",
      "Iteration 89100/200000, loss=0.7093654870986938\n",
      "Iteration 89200/200000, loss=0.6762229204177856\n",
      "Iteration 89300/200000, loss=0.6876784563064575\n",
      "Iteration 89400/200000, loss=0.6611278653144836\n",
      "Iteration 89500/200000, loss=0.6977472901344299\n",
      "Iteration 89600/200000, loss=0.7056620717048645\n",
      "Iteration 89700/200000, loss=0.7009366750717163\n",
      "Iteration 89800/200000, loss=0.6946113109588623\n",
      "Iteration 89900/200000, loss=0.70388263463974\n",
      "Iteration 90000/200000, loss=0.6986107230186462\n",
      "Iteration 90100/200000, loss=0.7057021260261536\n",
      "Iteration 90200/200000, loss=0.6455869674682617\n",
      "Iteration 90300/200000, loss=0.6663822531700134\n",
      "Iteration 90400/200000, loss=0.676353394985199\n",
      "Iteration 90500/200000, loss=0.6928462386131287\n",
      "Iteration 90600/200000, loss=0.6926854252815247\n",
      "Iteration 90700/200000, loss=0.67615807056427\n",
      "Iteration 90800/200000, loss=0.6978659629821777\n",
      "Iteration 90900/200000, loss=0.7467362284660339\n",
      "Iteration 91000/200000, loss=0.7253180742263794\n",
      "Iteration 91100/200000, loss=0.7051803469657898\n",
      "Iteration 91200/200000, loss=0.7360390424728394\n",
      "Iteration 91300/200000, loss=0.6788280606269836\n",
      "Iteration 91400/200000, loss=0.6560682654380798\n",
      "Iteration 91500/200000, loss=0.6328271627426147\n",
      "Iteration 91600/200000, loss=0.7176083922386169\n",
      "Iteration 91700/200000, loss=0.6672385334968567\n",
      "Iteration 91800/200000, loss=0.6793107986450195\n",
      "Iteration 91900/200000, loss=0.6972213387489319\n",
      "Iteration 92000/200000, loss=0.6733249425888062\n",
      "Iteration 92100/200000, loss=0.6697477698326111\n",
      "Iteration 92200/200000, loss=0.7024902105331421\n",
      "Iteration 92300/200000, loss=0.6374630928039551\n",
      "Iteration 92400/200000, loss=0.7268208861351013\n",
      "Iteration 92500/200000, loss=0.5967689752578735\n",
      "Iteration 92600/200000, loss=0.7036537528038025\n",
      "Iteration 92700/200000, loss=0.732629120349884\n",
      "Iteration 92800/200000, loss=0.6968119144439697\n",
      "Iteration 92900/200000, loss=0.6774610877037048\n",
      "Iteration 93000/200000, loss=0.6802238821983337\n",
      "Iteration 93100/200000, loss=0.6827663779258728\n",
      "Iteration 93200/200000, loss=0.6392309665679932\n",
      "Iteration 93300/200000, loss=0.6756710410118103\n",
      "Iteration 93400/200000, loss=0.6944342851638794\n",
      "Iteration 93500/200000, loss=0.706703782081604\n",
      "Iteration 93600/200000, loss=0.6699710488319397\n",
      "Iteration 93700/200000, loss=0.673859179019928\n",
      "Iteration 93800/200000, loss=0.7260861992835999\n",
      "Iteration 93900/200000, loss=0.7119215130805969\n",
      "Iteration 94000/200000, loss=0.7287499904632568\n",
      "Iteration 94100/200000, loss=0.6242390275001526\n",
      "Iteration 94200/200000, loss=0.6661897897720337\n",
      "Iteration 94300/200000, loss=0.7414899468421936\n",
      "Iteration 94400/200000, loss=0.7266567349433899\n",
      "Iteration 94500/200000, loss=0.7002885937690735\n",
      "Iteration 94600/200000, loss=0.6519898176193237\n",
      "Iteration 94700/200000, loss=0.6652218699455261\n",
      "Iteration 94800/200000, loss=0.6766822338104248\n",
      "Iteration 94900/200000, loss=0.7174341678619385\n",
      "Iteration 95000/200000, loss=0.6788982152938843\n",
      "Iteration 95100/200000, loss=0.6410049200057983\n",
      "Iteration 95200/200000, loss=0.6910932660102844\n",
      "Iteration 95300/200000, loss=0.6801828145980835\n",
      "Iteration 95400/200000, loss=0.7238162159919739\n",
      "Iteration 95500/200000, loss=0.6017720103263855\n",
      "Iteration 95600/200000, loss=0.6720200181007385\n",
      "Iteration 95700/200000, loss=0.6705369353294373\n",
      "Iteration 95800/200000, loss=0.6565119624137878\n",
      "Iteration 95900/200000, loss=0.7217526435852051\n",
      "Iteration 96000/200000, loss=0.6992717385292053\n",
      "Iteration 96100/200000, loss=0.7220180034637451\n",
      "Iteration 96200/200000, loss=0.658809244632721\n",
      "Iteration 96300/200000, loss=0.7342246174812317\n",
      "Iteration 96400/200000, loss=0.7172024250030518\n",
      "Iteration 96500/200000, loss=0.6914046406745911\n",
      "Iteration 96600/200000, loss=0.689790666103363\n",
      "Iteration 96700/200000, loss=0.6621755361557007\n",
      "Iteration 96800/200000, loss=0.6700595021247864\n",
      "Iteration 96900/200000, loss=0.6812887787818909\n",
      "Iteration 97000/200000, loss=0.7158359289169312\n",
      "Iteration 97100/200000, loss=0.6648616194725037\n",
      "Iteration 97200/200000, loss=0.7011153101921082\n",
      "Iteration 97300/200000, loss=0.6912925839424133\n",
      "Iteration 97400/200000, loss=0.7281829118728638\n",
      "Iteration 97500/200000, loss=0.7096591591835022\n",
      "Iteration 97600/200000, loss=0.6645280718803406\n",
      "Iteration 97700/200000, loss=0.7206092476844788\n",
      "Iteration 97800/200000, loss=0.6946074962615967\n",
      "Iteration 97900/200000, loss=0.7228822708129883\n",
      "Iteration 98000/200000, loss=0.6750708222389221\n",
      "Iteration 98100/200000, loss=0.6722152829170227\n",
      "Iteration 98200/200000, loss=0.7027051448822021\n",
      "Iteration 98300/200000, loss=0.6957942843437195\n",
      "Iteration 98400/200000, loss=0.7353914976119995\n",
      "Iteration 98500/200000, loss=0.6704739928245544\n",
      "Iteration 98600/200000, loss=0.6658121347427368\n",
      "Iteration 98700/200000, loss=0.6787323355674744\n",
      "Iteration 98800/200000, loss=0.6941967606544495\n",
      "Iteration 98900/200000, loss=0.7081338167190552\n",
      "Iteration 99000/200000, loss=0.6814621686935425\n",
      "Iteration 99100/200000, loss=0.6850904226303101\n",
      "Iteration 99200/200000, loss=0.690048098564148\n",
      "Iteration 99300/200000, loss=0.6779077053070068\n",
      "Iteration 99400/200000, loss=0.6964395642280579\n",
      "Iteration 99500/200000, loss=0.6825796365737915\n",
      "Iteration 99600/200000, loss=0.6865783929824829\n",
      "Iteration 99700/200000, loss=0.690430223941803\n",
      "Iteration 99800/200000, loss=0.6711106300354004\n",
      "Iteration 99900/200000, loss=0.7131309509277344\n",
      "Iteration 100000/200000, loss=0.6902417540550232\n",
      "Iteration 100100/200000, loss=0.6954881548881531\n",
      "Iteration 100200/200000, loss=0.6913896799087524\n",
      "Iteration 100300/200000, loss=0.6644216775894165\n",
      "Iteration 100400/200000, loss=0.7128543853759766\n",
      "Iteration 100500/200000, loss=0.6931723356246948\n",
      "Iteration 100600/200000, loss=0.6985724568367004\n",
      "Iteration 100700/200000, loss=0.6920514106750488\n",
      "Iteration 100800/200000, loss=0.7604460120201111\n",
      "Iteration 100900/200000, loss=0.717149555683136\n",
      "Iteration 101000/200000, loss=0.7004699110984802\n",
      "Iteration 101100/200000, loss=0.6598316431045532\n",
      "Iteration 101200/200000, loss=0.6824274659156799\n",
      "Iteration 101300/200000, loss=0.6979696154594421\n",
      "Iteration 101400/200000, loss=0.6850537657737732\n",
      "Iteration 101500/200000, loss=0.657846987247467\n",
      "Iteration 101600/200000, loss=0.6769942045211792\n",
      "Iteration 101700/200000, loss=0.7433756589889526\n",
      "Iteration 101800/200000, loss=0.6931582689285278\n",
      "Iteration 101900/200000, loss=0.6622753143310547\n",
      "Iteration 102000/200000, loss=0.7089656591415405\n",
      "Iteration 102100/200000, loss=0.674152135848999\n",
      "Iteration 102200/200000, loss=0.6526319980621338\n",
      "Iteration 102300/200000, loss=0.7436240911483765\n",
      "Iteration 102400/200000, loss=0.6451864242553711\n",
      "Iteration 102500/200000, loss=0.7322356104850769\n",
      "Iteration 102600/200000, loss=0.671539306640625\n",
      "Iteration 102700/200000, loss=0.6839815974235535\n",
      "Iteration 102800/200000, loss=0.6650994420051575\n",
      "Iteration 102900/200000, loss=0.6747830510139465\n",
      "Iteration 103000/200000, loss=0.7209901213645935\n",
      "Iteration 103100/200000, loss=0.7414318323135376\n",
      "Iteration 103200/200000, loss=0.6648885011672974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 103300/200000, loss=0.6475465297698975\n",
      "Iteration 103400/200000, loss=0.6671529412269592\n",
      "Iteration 103500/200000, loss=0.7530296444892883\n",
      "Iteration 103600/200000, loss=0.7571340203285217\n",
      "Iteration 103700/200000, loss=0.6482071876525879\n",
      "Iteration 103800/200000, loss=0.7641058564186096\n",
      "Iteration 103900/200000, loss=0.6688756346702576\n",
      "Iteration 104000/200000, loss=0.6420685648918152\n",
      "Iteration 104100/200000, loss=0.7452533841133118\n",
      "Iteration 104200/200000, loss=0.6631569862365723\n",
      "Iteration 104300/200000, loss=0.7395238280296326\n",
      "Iteration 104400/200000, loss=0.7063288688659668\n",
      "Iteration 104500/200000, loss=0.6754046678543091\n",
      "Iteration 104600/200000, loss=0.7373908758163452\n",
      "Iteration 104700/200000, loss=0.7568931579589844\n",
      "Iteration 104800/200000, loss=0.6777749061584473\n",
      "Iteration 104900/200000, loss=0.7284717559814453\n",
      "Iteration 105000/200000, loss=0.6597802639007568\n",
      "Iteration 105100/200000, loss=0.6806949377059937\n",
      "Iteration 105200/200000, loss=0.7204013466835022\n",
      "Iteration 105300/200000, loss=0.7392320036888123\n",
      "Iteration 105400/200000, loss=0.7011585831642151\n",
      "Iteration 105500/200000, loss=0.6945792436599731\n",
      "Iteration 105600/200000, loss=0.6836917400360107\n",
      "Iteration 105700/200000, loss=0.7065569758415222\n",
      "Iteration 105800/200000, loss=0.7091978788375854\n",
      "Iteration 105900/200000, loss=0.7084319591522217\n",
      "Iteration 106000/200000, loss=0.7109291553497314\n",
      "Iteration 106100/200000, loss=0.710695207118988\n",
      "Iteration 106200/200000, loss=0.6563353538513184\n",
      "Iteration 106300/200000, loss=0.6797530651092529\n",
      "Iteration 106400/200000, loss=0.6934637427330017\n",
      "Iteration 106500/200000, loss=0.6651676297187805\n",
      "Iteration 106600/200000, loss=0.7240831851959229\n",
      "Iteration 106700/200000, loss=0.6998497247695923\n",
      "Iteration 106800/200000, loss=0.7253711819648743\n",
      "Iteration 106900/200000, loss=0.6619131565093994\n",
      "Iteration 107000/200000, loss=0.6866888999938965\n",
      "Iteration 107100/200000, loss=0.6822587847709656\n",
      "Iteration 107200/200000, loss=0.6923007965087891\n",
      "Iteration 107300/200000, loss=0.6049610376358032\n",
      "Iteration 107400/200000, loss=0.6848629117012024\n",
      "Iteration 107500/200000, loss=0.6775103807449341\n",
      "Iteration 107600/200000, loss=0.7089155912399292\n",
      "Iteration 107700/200000, loss=0.6834090352058411\n",
      "Iteration 107800/200000, loss=0.7019426226615906\n",
      "Iteration 107900/200000, loss=0.6800408959388733\n",
      "Iteration 108000/200000, loss=0.7059530019760132\n",
      "Iteration 108100/200000, loss=0.7039753794670105\n",
      "Iteration 108200/200000, loss=0.6433133482933044\n",
      "Iteration 108300/200000, loss=0.7035713195800781\n",
      "Iteration 108400/200000, loss=0.726540744304657\n",
      "Iteration 108500/200000, loss=0.7127391695976257\n",
      "Iteration 108600/200000, loss=0.693028450012207\n",
      "Iteration 108700/200000, loss=0.6683040857315063\n",
      "Iteration 108800/200000, loss=0.6281174421310425\n",
      "Iteration 108900/200000, loss=0.6843981146812439\n",
      "Iteration 109000/200000, loss=0.7133356928825378\n",
      "Iteration 109100/200000, loss=0.6730095744132996\n",
      "Iteration 109200/200000, loss=0.7574021816253662\n",
      "Iteration 109300/200000, loss=0.5879782438278198\n",
      "Iteration 109400/200000, loss=0.6876922845840454\n",
      "Iteration 109500/200000, loss=0.6913107633590698\n",
      "Iteration 109600/200000, loss=0.7014048099517822\n",
      "Iteration 109700/200000, loss=0.7342689633369446\n",
      "Iteration 109800/200000, loss=0.6984544992446899\n",
      "Iteration 109900/200000, loss=0.6814781427383423\n",
      "Iteration 110000/200000, loss=0.6769511699676514\n",
      "Iteration 110100/200000, loss=0.7021792531013489\n",
      "Iteration 110200/200000, loss=0.6735461354255676\n",
      "Iteration 110300/200000, loss=0.6753883361816406\n",
      "Iteration 110400/200000, loss=0.6906260251998901\n",
      "Iteration 110500/200000, loss=0.7214493751525879\n",
      "Iteration 110600/200000, loss=0.698736846446991\n",
      "Iteration 110700/200000, loss=0.7296878695487976\n",
      "Iteration 110800/200000, loss=0.7012356519699097\n",
      "Iteration 110900/200000, loss=0.6621323227882385\n",
      "Iteration 111000/200000, loss=0.6906949877738953\n",
      "Iteration 111100/200000, loss=0.6980976462364197\n",
      "Iteration 111200/200000, loss=0.6775702238082886\n",
      "Iteration 111300/200000, loss=0.7198799848556519\n",
      "Iteration 111400/200000, loss=0.6775860786437988\n",
      "Iteration 111500/200000, loss=0.6992312669754028\n",
      "Iteration 111600/200000, loss=0.7087968587875366\n",
      "Iteration 111700/200000, loss=0.6819475889205933\n",
      "Iteration 111800/200000, loss=0.666048526763916\n",
      "Iteration 111900/200000, loss=0.6721629500389099\n",
      "Iteration 112000/200000, loss=0.6689655780792236\n",
      "Iteration 112100/200000, loss=0.6593364477157593\n",
      "Iteration 112200/200000, loss=0.7000240683555603\n",
      "Iteration 112300/200000, loss=0.7124834060668945\n",
      "Iteration 112400/200000, loss=0.6871262788772583\n",
      "Iteration 112500/200000, loss=0.6948561668395996\n",
      "Iteration 112600/200000, loss=0.6928138136863708\n",
      "Iteration 112700/200000, loss=0.6337279081344604\n",
      "Iteration 112800/200000, loss=0.6847378015518188\n",
      "Iteration 112900/200000, loss=0.7156285047531128\n",
      "Iteration 113000/200000, loss=0.7115533947944641\n",
      "Iteration 113100/200000, loss=0.7235037684440613\n",
      "Iteration 113200/200000, loss=0.675637423992157\n",
      "Iteration 113300/200000, loss=0.7212441563606262\n",
      "Iteration 113400/200000, loss=0.6629160046577454\n",
      "Iteration 113500/200000, loss=0.7263238430023193\n",
      "Iteration 113600/200000, loss=0.7154330015182495\n",
      "Iteration 113700/200000, loss=0.6791065335273743\n",
      "Iteration 113800/200000, loss=0.6993324160575867\n",
      "Iteration 113900/200000, loss=0.7278814911842346\n",
      "Iteration 114000/200000, loss=0.7398272752761841\n",
      "Iteration 114100/200000, loss=0.6606041789054871\n",
      "Iteration 114200/200000, loss=0.6538141965866089\n",
      "Iteration 114300/200000, loss=0.7466714382171631\n",
      "Iteration 114400/200000, loss=0.6465057730674744\n",
      "Iteration 114500/200000, loss=0.6837404370307922\n",
      "Iteration 114600/200000, loss=0.7009033560752869\n",
      "Iteration 114700/200000, loss=0.7251519560813904\n",
      "Iteration 114800/200000, loss=0.6985790133476257\n",
      "Iteration 114900/200000, loss=0.6619648933410645\n",
      "Iteration 115000/200000, loss=0.6956877708435059\n",
      "Iteration 115100/200000, loss=0.717765212059021\n",
      "Iteration 115200/200000, loss=0.6604042053222656\n",
      "Iteration 115300/200000, loss=0.750832736492157\n",
      "Iteration 115400/200000, loss=0.6745079159736633\n",
      "Iteration 115500/200000, loss=0.6237348318099976\n",
      "Iteration 115600/200000, loss=0.6908895969390869\n",
      "Iteration 115700/200000, loss=0.6727802157402039\n",
      "Iteration 115800/200000, loss=0.7043149471282959\n",
      "Iteration 115900/200000, loss=0.7069955468177795\n",
      "Iteration 116000/200000, loss=0.6648157238960266\n",
      "Iteration 116100/200000, loss=0.7472675442695618\n",
      "Iteration 116200/200000, loss=0.6919052004814148\n",
      "Iteration 116300/200000, loss=0.6726702451705933\n",
      "Iteration 116400/200000, loss=0.728779137134552\n",
      "Iteration 116500/200000, loss=0.6939576268196106\n",
      "Iteration 116600/200000, loss=0.6996114253997803\n",
      "Iteration 116700/200000, loss=0.7318194508552551\n",
      "Iteration 116800/200000, loss=0.6751204133033752\n",
      "Iteration 116900/200000, loss=0.7065102458000183\n",
      "Iteration 117000/200000, loss=0.6853923201560974\n",
      "Iteration 117100/200000, loss=0.7292681932449341\n",
      "Iteration 117200/200000, loss=0.6860338449478149\n",
      "Iteration 117300/200000, loss=0.6715232729911804\n",
      "Iteration 117400/200000, loss=0.7046789526939392\n",
      "Iteration 117500/200000, loss=0.6985540390014648\n",
      "Iteration 117600/200000, loss=0.6876055002212524\n",
      "Iteration 117700/200000, loss=0.6860009431838989\n",
      "Iteration 117800/200000, loss=0.6499992609024048\n",
      "Iteration 117900/200000, loss=0.7401468753814697\n",
      "Iteration 118000/200000, loss=0.7033241391181946\n",
      "Iteration 118100/200000, loss=0.716197669506073\n",
      "Iteration 118200/200000, loss=0.6612887978553772\n",
      "Iteration 118300/200000, loss=0.6838036775588989\n",
      "Iteration 118400/200000, loss=0.6265283823013306\n",
      "Iteration 118500/200000, loss=0.6668621301651001\n",
      "Iteration 118600/200000, loss=0.6481719613075256\n",
      "Iteration 118700/200000, loss=0.6979049444198608\n",
      "Iteration 118800/200000, loss=0.6174311637878418\n",
      "Iteration 118900/200000, loss=0.6622233986854553\n",
      "Iteration 119000/200000, loss=0.6788324117660522\n",
      "Iteration 119100/200000, loss=0.6643694639205933\n",
      "Iteration 119200/200000, loss=0.7018117904663086\n",
      "Iteration 119300/200000, loss=0.7371461391448975\n",
      "Iteration 119400/200000, loss=0.7110866904258728\n",
      "Iteration 119500/200000, loss=0.6759872436523438\n",
      "Iteration 119600/200000, loss=0.7040972709655762\n",
      "Iteration 119700/200000, loss=0.6941720843315125\n",
      "Iteration 119800/200000, loss=0.654687762260437\n",
      "Iteration 119900/200000, loss=0.7084522843360901\n",
      "Iteration 120000/200000, loss=0.7415974140167236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 120100/200000, loss=0.6772424578666687\n",
      "Iteration 120200/200000, loss=0.6844866275787354\n",
      "Iteration 120300/200000, loss=0.7163321375846863\n",
      "Iteration 120400/200000, loss=0.7213020324707031\n",
      "Iteration 120500/200000, loss=0.6744191646575928\n",
      "Iteration 120600/200000, loss=0.7023179531097412\n",
      "Iteration 120700/200000, loss=0.6943390369415283\n",
      "Iteration 120800/200000, loss=0.6771208047866821\n",
      "Iteration 120900/200000, loss=0.6884981989860535\n",
      "Iteration 121000/200000, loss=0.6875671148300171\n",
      "Iteration 121100/200000, loss=0.6917904615402222\n",
      "Iteration 121200/200000, loss=0.6919592618942261\n",
      "Iteration 121300/200000, loss=0.6895256042480469\n",
      "Iteration 121400/200000, loss=0.6757872104644775\n",
      "Iteration 121500/200000, loss=0.7076008319854736\n",
      "Iteration 121600/200000, loss=0.7080901265144348\n",
      "Iteration 121700/200000, loss=0.6492502689361572\n",
      "Iteration 121800/200000, loss=0.7180042266845703\n",
      "Iteration 121900/200000, loss=0.7020112872123718\n",
      "Iteration 122000/200000, loss=0.6341026425361633\n",
      "Iteration 122100/200000, loss=0.6868733763694763\n",
      "Iteration 122200/200000, loss=0.7153639197349548\n",
      "Iteration 122300/200000, loss=0.6728407740592957\n",
      "Iteration 122400/200000, loss=0.7259193658828735\n",
      "Iteration 122500/200000, loss=0.7894879579544067\n",
      "Iteration 122600/200000, loss=0.6649890542030334\n",
      "Iteration 122700/200000, loss=0.7030641436576843\n",
      "Iteration 122800/200000, loss=0.7283105254173279\n",
      "Iteration 122900/200000, loss=0.6332094073295593\n",
      "Iteration 123000/200000, loss=0.66445392370224\n",
      "Iteration 123100/200000, loss=0.6749240756034851\n",
      "Iteration 123200/200000, loss=0.7178124189376831\n",
      "Iteration 123300/200000, loss=0.6843577027320862\n",
      "Iteration 123400/200000, loss=0.7496109008789062\n",
      "Iteration 123500/200000, loss=0.657683789730072\n",
      "Iteration 123600/200000, loss=0.6689825654029846\n",
      "Iteration 123700/200000, loss=0.7201892733573914\n",
      "Iteration 123800/200000, loss=0.7026766538619995\n",
      "Iteration 123900/200000, loss=0.7367959022521973\n",
      "Iteration 124000/200000, loss=0.6844177842140198\n",
      "Iteration 124100/200000, loss=0.7305105924606323\n",
      "Iteration 124200/200000, loss=0.7536112666130066\n",
      "Iteration 124300/200000, loss=0.678591787815094\n",
      "Iteration 124400/200000, loss=0.7407058477401733\n",
      "Iteration 124500/200000, loss=0.6568800210952759\n",
      "Iteration 124600/200000, loss=0.7175792455673218\n",
      "Iteration 124700/200000, loss=0.6701697707176208\n",
      "Iteration 124800/200000, loss=0.7165958881378174\n",
      "Iteration 124900/200000, loss=0.6894938945770264\n",
      "Iteration 125000/200000, loss=0.6807386875152588\n",
      "Iteration 125100/200000, loss=0.713381290435791\n",
      "Iteration 125200/200000, loss=0.7221734523773193\n",
      "Iteration 125300/200000, loss=0.7258710861206055\n",
      "Iteration 125400/200000, loss=0.6438528895378113\n",
      "Iteration 125500/200000, loss=0.7320054769515991\n",
      "Iteration 125600/200000, loss=0.7140383720397949\n",
      "Iteration 125700/200000, loss=0.6516302227973938\n",
      "Iteration 125800/200000, loss=0.6947944760322571\n",
      "Iteration 125900/200000, loss=0.6975207328796387\n",
      "Iteration 126000/200000, loss=0.6709919571876526\n",
      "Iteration 126100/200000, loss=0.6415306925773621\n",
      "Iteration 126200/200000, loss=0.7009676694869995\n",
      "Iteration 126300/200000, loss=0.6462461948394775\n",
      "Iteration 126400/200000, loss=0.6522358655929565\n",
      "Iteration 126500/200000, loss=0.6985330581665039\n",
      "Iteration 126600/200000, loss=0.7157462239265442\n",
      "Iteration 126700/200000, loss=0.6976109743118286\n",
      "Iteration 126800/200000, loss=0.6670802235603333\n",
      "Iteration 126900/200000, loss=0.6781712174415588\n",
      "Iteration 127000/200000, loss=0.6869056820869446\n",
      "Iteration 127100/200000, loss=0.6703135967254639\n",
      "Iteration 127200/200000, loss=0.6907018423080444\n",
      "Iteration 127300/200000, loss=0.7336692214012146\n",
      "Iteration 127400/200000, loss=0.7225388288497925\n",
      "Iteration 127500/200000, loss=0.672757089138031\n",
      "Iteration 127600/200000, loss=0.7628229856491089\n",
      "Iteration 127700/200000, loss=0.6635467410087585\n",
      "Iteration 127800/200000, loss=0.6717857718467712\n",
      "Iteration 127900/200000, loss=0.7247403860092163\n",
      "Iteration 128000/200000, loss=0.6618995666503906\n",
      "Iteration 128100/200000, loss=0.687736988067627\n",
      "Iteration 128200/200000, loss=0.7348091006278992\n",
      "Iteration 128300/200000, loss=0.7058516144752502\n",
      "Iteration 128400/200000, loss=0.6994364261627197\n",
      "Iteration 128500/200000, loss=0.6734963059425354\n",
      "Iteration 128600/200000, loss=0.7225760817527771\n",
      "Iteration 128700/200000, loss=0.7161064743995667\n",
      "Iteration 128800/200000, loss=0.7199215888977051\n",
      "Iteration 128900/200000, loss=0.7448064088821411\n",
      "Iteration 129000/200000, loss=0.6199044585227966\n",
      "Iteration 129100/200000, loss=0.6694673895835876\n",
      "Iteration 129200/200000, loss=0.6642351746559143\n",
      "Iteration 129300/200000, loss=0.6687103509902954\n",
      "Iteration 129400/200000, loss=0.7436777949333191\n",
      "Iteration 129500/200000, loss=0.6722281575202942\n",
      "Iteration 129600/200000, loss=0.7019292712211609\n",
      "Iteration 129700/200000, loss=0.646553099155426\n",
      "Iteration 129800/200000, loss=0.7009084224700928\n",
      "Iteration 129900/200000, loss=0.6770521998405457\n",
      "Iteration 130000/200000, loss=0.6700630187988281\n",
      "Iteration 130100/200000, loss=0.7168309688568115\n",
      "Iteration 130200/200000, loss=0.7221108675003052\n",
      "Iteration 130300/200000, loss=0.670237123966217\n",
      "Iteration 130400/200000, loss=0.6495633125305176\n",
      "Iteration 130500/200000, loss=0.6942736506462097\n",
      "Iteration 130600/200000, loss=0.681701123714447\n",
      "Iteration 130700/200000, loss=0.6822319626808167\n",
      "Iteration 130800/200000, loss=0.6974210143089294\n",
      "Iteration 130900/200000, loss=0.6221617460250854\n",
      "Iteration 131000/200000, loss=0.7024981379508972\n",
      "Iteration 131100/200000, loss=0.7452824115753174\n",
      "Iteration 131200/200000, loss=0.6830240488052368\n",
      "Iteration 131300/200000, loss=0.6980897188186646\n",
      "Iteration 131400/200000, loss=0.7130970358848572\n",
      "Iteration 131500/200000, loss=0.7236714959144592\n",
      "Iteration 131600/200000, loss=0.7241442799568176\n",
      "Iteration 131700/200000, loss=0.7150694727897644\n",
      "Iteration 131800/200000, loss=0.7027084827423096\n",
      "Iteration 131900/200000, loss=0.679780900478363\n",
      "Iteration 132000/200000, loss=0.7101870775222778\n",
      "Iteration 132100/200000, loss=0.6943756937980652\n",
      "Iteration 132200/200000, loss=0.6411527395248413\n",
      "Iteration 132300/200000, loss=0.6945935487747192\n",
      "Iteration 132400/200000, loss=0.6445308923721313\n",
      "Iteration 132500/200000, loss=0.6575466990470886\n",
      "Iteration 132600/200000, loss=0.6344032883644104\n",
      "Iteration 132700/200000, loss=0.664743185043335\n",
      "Iteration 132800/200000, loss=0.7006797194480896\n",
      "Iteration 132900/200000, loss=0.6796976327896118\n",
      "Iteration 133000/200000, loss=0.726961612701416\n",
      "Iteration 133100/200000, loss=0.644627034664154\n",
      "Iteration 133200/200000, loss=0.6873891353607178\n",
      "Iteration 133300/200000, loss=0.6897993683815002\n",
      "Iteration 133400/200000, loss=0.6782645583152771\n",
      "Iteration 133500/200000, loss=0.6692159175872803\n",
      "Iteration 133600/200000, loss=0.6988875269889832\n",
      "Iteration 133700/200000, loss=0.6964027285575867\n",
      "Iteration 133800/200000, loss=0.6984145641326904\n",
      "Iteration 133900/200000, loss=0.720395028591156\n",
      "Iteration 134000/200000, loss=0.6188234090805054\n",
      "Iteration 134100/200000, loss=0.7224159836769104\n",
      "Iteration 134200/200000, loss=0.7226091623306274\n",
      "Iteration 134300/200000, loss=0.6830252408981323\n",
      "Iteration 134400/200000, loss=0.672581136226654\n",
      "Iteration 134500/200000, loss=0.6934954524040222\n",
      "Iteration 134600/200000, loss=0.6520894765853882\n",
      "Iteration 134700/200000, loss=0.6276617050170898\n",
      "Iteration 134800/200000, loss=0.6929813623428345\n",
      "Iteration 134900/200000, loss=0.7204386591911316\n",
      "Iteration 135000/200000, loss=0.746918797492981\n",
      "Iteration 135100/200000, loss=0.7048887014389038\n",
      "Iteration 135200/200000, loss=0.7414951920509338\n",
      "Iteration 135300/200000, loss=0.6854740977287292\n",
      "Iteration 135400/200000, loss=0.7136043310165405\n",
      "Iteration 135500/200000, loss=0.6672144532203674\n",
      "Iteration 135600/200000, loss=0.7030823826789856\n",
      "Iteration 135700/200000, loss=0.7210980653762817\n",
      "Iteration 135800/200000, loss=0.7165607810020447\n",
      "Iteration 135900/200000, loss=0.6868458390235901\n",
      "Iteration 136000/200000, loss=0.6994374394416809\n",
      "Iteration 136100/200000, loss=0.6596699357032776\n",
      "Iteration 136200/200000, loss=0.7004441022872925\n",
      "Iteration 136300/200000, loss=0.6843839883804321\n",
      "Iteration 136400/200000, loss=0.6938306093215942\n",
      "Iteration 136500/200000, loss=0.7050255537033081\n",
      "Iteration 136600/200000, loss=0.7130120396614075\n",
      "Iteration 136700/200000, loss=0.7071277499198914\n",
      "Iteration 136800/200000, loss=0.6773897409439087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 136900/200000, loss=0.6610421538352966\n",
      "Iteration 137000/200000, loss=0.7297428846359253\n",
      "Iteration 137100/200000, loss=0.7489959001541138\n",
      "Iteration 137200/200000, loss=0.7097965478897095\n",
      "Iteration 137300/200000, loss=0.663169264793396\n",
      "Iteration 137400/200000, loss=0.6377471685409546\n",
      "Iteration 137500/200000, loss=0.5835635662078857\n",
      "Iteration 137600/200000, loss=0.6749016046524048\n",
      "Iteration 137700/200000, loss=0.7295860052108765\n",
      "Iteration 137800/200000, loss=0.6569421291351318\n",
      "Iteration 137900/200000, loss=0.731208860874176\n",
      "Iteration 138000/200000, loss=0.6742280721664429\n",
      "Iteration 138100/200000, loss=0.7294996380805969\n",
      "Iteration 138200/200000, loss=0.6530367136001587\n",
      "Iteration 138300/200000, loss=0.6751075983047485\n",
      "Iteration 138400/200000, loss=0.7382983565330505\n",
      "Iteration 138500/200000, loss=0.6802161335945129\n",
      "Iteration 138600/200000, loss=0.6894387602806091\n",
      "Iteration 138700/200000, loss=0.7251121401786804\n",
      "Iteration 138800/200000, loss=0.6687840223312378\n",
      "Iteration 138900/200000, loss=0.7483822107315063\n",
      "Iteration 139000/200000, loss=0.6780099272727966\n",
      "Iteration 139100/200000, loss=0.6797565817832947\n",
      "Iteration 139200/200000, loss=0.720436692237854\n",
      "Iteration 139300/200000, loss=0.679498553276062\n",
      "Iteration 139400/200000, loss=0.7025002837181091\n",
      "Iteration 139500/200000, loss=0.7121075987815857\n",
      "Iteration 139600/200000, loss=0.650749146938324\n",
      "Iteration 139700/200000, loss=0.676419734954834\n",
      "Iteration 139800/200000, loss=0.6422573328018188\n",
      "Iteration 139900/200000, loss=0.6889668703079224\n",
      "Iteration 140000/200000, loss=0.6777189373970032\n",
      "Iteration 140100/200000, loss=0.7160171270370483\n",
      "Iteration 140200/200000, loss=0.784924328327179\n",
      "Iteration 140300/200000, loss=0.702727735042572\n",
      "Iteration 140400/200000, loss=0.7285353541374207\n",
      "Iteration 140500/200000, loss=0.6963881254196167\n",
      "Iteration 140600/200000, loss=0.6642489433288574\n",
      "Iteration 140700/200000, loss=0.7332270741462708\n",
      "Iteration 140800/200000, loss=0.4883839190006256\n",
      "Iteration 140900/200000, loss=0.7106139063835144\n",
      "Iteration 141000/200000, loss=0.7008095383644104\n",
      "Iteration 141100/200000, loss=0.6645460724830627\n",
      "Iteration 141200/200000, loss=0.6560136079788208\n",
      "Iteration 141300/200000, loss=0.6588901877403259\n",
      "Iteration 141400/200000, loss=0.6726616621017456\n",
      "Iteration 141500/200000, loss=0.717000424861908\n",
      "Iteration 141600/200000, loss=0.6752195954322815\n",
      "Iteration 141700/200000, loss=0.7127813696861267\n",
      "Iteration 141800/200000, loss=0.7093663811683655\n",
      "Iteration 141900/200000, loss=0.6776628494262695\n",
      "Iteration 142000/200000, loss=0.7187445759773254\n",
      "Iteration 142100/200000, loss=0.6153625845909119\n",
      "Iteration 142200/200000, loss=0.6905015707015991\n",
      "Iteration 142300/200000, loss=0.6380335688591003\n",
      "Iteration 142400/200000, loss=0.7020212411880493\n",
      "Iteration 142500/200000, loss=0.6879966259002686\n",
      "Iteration 142600/200000, loss=0.6453630328178406\n",
      "Iteration 142700/200000, loss=0.6427449584007263\n",
      "Iteration 142800/200000, loss=0.7365472316741943\n",
      "Iteration 142900/200000, loss=0.6460697054862976\n",
      "Iteration 143000/200000, loss=0.7414217591285706\n",
      "Iteration 143100/200000, loss=0.7060533165931702\n",
      "Iteration 143200/200000, loss=0.6924322843551636\n",
      "Iteration 143300/200000, loss=0.7755144238471985\n",
      "Iteration 143400/200000, loss=0.6735666394233704\n",
      "Iteration 143500/200000, loss=0.6854754686355591\n",
      "Iteration 143600/200000, loss=0.6817790269851685\n",
      "Iteration 143700/200000, loss=0.7051276564598083\n",
      "Iteration 143800/200000, loss=0.7279677987098694\n",
      "Iteration 143900/200000, loss=0.6925410628318787\n",
      "Iteration 144000/200000, loss=0.6833863854408264\n",
      "Iteration 144100/200000, loss=0.6700419783592224\n",
      "Iteration 144200/200000, loss=0.6750279664993286\n",
      "Iteration 144300/200000, loss=0.7406319975852966\n",
      "Iteration 144400/200000, loss=0.6871001720428467\n",
      "Iteration 144500/200000, loss=0.7192411422729492\n",
      "Iteration 144600/200000, loss=0.7380579710006714\n",
      "Iteration 144700/200000, loss=0.7062627077102661\n",
      "Iteration 144800/200000, loss=0.6427984833717346\n",
      "Iteration 144900/200000, loss=0.7446708679199219\n",
      "Iteration 145000/200000, loss=0.6707140803337097\n",
      "Iteration 145100/200000, loss=0.6853669285774231\n",
      "Iteration 145200/200000, loss=0.722281813621521\n",
      "Iteration 145300/200000, loss=0.7064590454101562\n",
      "Iteration 145400/200000, loss=0.6711372137069702\n",
      "Iteration 145500/200000, loss=0.7161188721656799\n",
      "Iteration 145600/200000, loss=0.7167502045631409\n",
      "Iteration 145700/200000, loss=0.7380494475364685\n",
      "Iteration 145800/200000, loss=0.7321406006813049\n",
      "Iteration 145900/200000, loss=0.7130568027496338\n",
      "Iteration 146000/200000, loss=0.6622183918952942\n",
      "Iteration 146100/200000, loss=0.6974271535873413\n",
      "Iteration 146200/200000, loss=0.729288637638092\n",
      "Iteration 146300/200000, loss=0.6674143075942993\n",
      "Iteration 146400/200000, loss=0.6812928318977356\n",
      "Iteration 146500/200000, loss=0.6936031579971313\n",
      "Iteration 146600/200000, loss=0.6750749945640564\n",
      "Iteration 146700/200000, loss=0.6767861247062683\n",
      "Iteration 146800/200000, loss=0.7413560152053833\n",
      "Iteration 146900/200000, loss=0.6877726912498474\n",
      "Iteration 147000/200000, loss=0.6774845719337463\n",
      "Iteration 147100/200000, loss=0.7223843932151794\n",
      "Iteration 147200/200000, loss=0.7431719303131104\n",
      "Iteration 147300/200000, loss=0.6798383593559265\n",
      "Iteration 147400/200000, loss=0.6824821829795837\n",
      "Iteration 147500/200000, loss=0.7142447233200073\n",
      "Iteration 147600/200000, loss=0.6598203778266907\n",
      "Iteration 147700/200000, loss=0.6542269587516785\n",
      "Iteration 147800/200000, loss=0.6764042973518372\n",
      "Iteration 147900/200000, loss=0.7283768057823181\n",
      "Iteration 148000/200000, loss=0.6653742790222168\n",
      "Iteration 148100/200000, loss=0.6819387078285217\n",
      "Iteration 148200/200000, loss=0.6330090165138245\n",
      "Iteration 148300/200000, loss=0.6822840571403503\n",
      "Iteration 148400/200000, loss=0.6799452900886536\n",
      "Iteration 148500/200000, loss=0.7517720460891724\n",
      "Iteration 148600/200000, loss=0.6755341291427612\n",
      "Iteration 148700/200000, loss=0.6963828802108765\n",
      "Iteration 148800/200000, loss=0.6717487573623657\n",
      "Iteration 148900/200000, loss=0.6767745614051819\n",
      "Iteration 149000/200000, loss=0.6890198588371277\n",
      "Iteration 149100/200000, loss=0.7508817315101624\n",
      "Iteration 149200/200000, loss=0.6795794367790222\n",
      "Iteration 149300/200000, loss=0.6950477361679077\n",
      "Iteration 149400/200000, loss=0.6902427077293396\n",
      "Iteration 149500/200000, loss=0.6560136079788208\n",
      "Iteration 149600/200000, loss=0.7263507843017578\n",
      "Iteration 149700/200000, loss=0.6304070949554443\n",
      "Iteration 149800/200000, loss=0.7180290222167969\n",
      "Iteration 149900/200000, loss=0.6958869695663452\n",
      "Iteration 150000/200000, loss=0.6949034333229065\n",
      "Iteration 150100/200000, loss=0.6722107529640198\n",
      "Iteration 150200/200000, loss=0.6542502045631409\n",
      "Iteration 150300/200000, loss=0.6916929483413696\n",
      "Iteration 150400/200000, loss=0.7025468945503235\n",
      "Iteration 150500/200000, loss=0.7000842094421387\n",
      "Iteration 150600/200000, loss=0.7098979949951172\n",
      "Iteration 150700/200000, loss=0.6630216836929321\n",
      "Iteration 150800/200000, loss=0.7342377305030823\n",
      "Iteration 150900/200000, loss=0.697568416595459\n",
      "Iteration 151000/200000, loss=0.7192530632019043\n",
      "Iteration 151100/200000, loss=0.7512216567993164\n",
      "Iteration 151200/200000, loss=0.7754518389701843\n",
      "Iteration 151300/200000, loss=0.698410153388977\n",
      "Iteration 151400/200000, loss=0.7144182324409485\n",
      "Iteration 151500/200000, loss=0.6879057884216309\n",
      "Iteration 151600/200000, loss=0.7431256771087646\n",
      "Iteration 151700/200000, loss=0.7159937024116516\n",
      "Iteration 151800/200000, loss=0.7138109803199768\n",
      "Iteration 151900/200000, loss=0.6275758147239685\n",
      "Iteration 152000/200000, loss=0.7149003148078918\n",
      "Iteration 152100/200000, loss=0.7012632489204407\n",
      "Iteration 152200/200000, loss=0.7371877431869507\n",
      "Iteration 152300/200000, loss=0.7147363424301147\n",
      "Iteration 152400/200000, loss=0.7134875059127808\n",
      "Iteration 152500/200000, loss=0.698884904384613\n",
      "Iteration 152600/200000, loss=0.6859510540962219\n",
      "Iteration 152700/200000, loss=0.7252287864685059\n",
      "Iteration 152800/200000, loss=0.6871634125709534\n",
      "Iteration 152900/200000, loss=0.7498794794082642\n",
      "Iteration 153000/200000, loss=0.6775178909301758\n",
      "Iteration 153100/200000, loss=0.6374791860580444\n",
      "Iteration 153200/200000, loss=0.6230062246322632\n",
      "Iteration 153300/200000, loss=0.6510007381439209\n",
      "Iteration 153400/200000, loss=0.6334100365638733\n",
      "Iteration 153500/200000, loss=0.6930601596832275\n",
      "Iteration 153600/200000, loss=0.5990709066390991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 153700/200000, loss=0.6994280815124512\n",
      "Iteration 153800/200000, loss=0.6857548356056213\n",
      "Iteration 153900/200000, loss=0.6407324075698853\n",
      "Iteration 154000/200000, loss=0.716506838798523\n",
      "Iteration 154100/200000, loss=0.7279421091079712\n",
      "Iteration 154200/200000, loss=0.7427209615707397\n",
      "Iteration 154300/200000, loss=0.6812722086906433\n",
      "Iteration 154400/200000, loss=0.6626780033111572\n",
      "Iteration 154500/200000, loss=0.6933448314666748\n",
      "Iteration 154600/200000, loss=0.6478520035743713\n",
      "Iteration 154700/200000, loss=0.7106404304504395\n",
      "Iteration 154800/200000, loss=0.713575541973114\n",
      "Iteration 154900/200000, loss=0.6559085249900818\n",
      "Iteration 155000/200000, loss=0.6775311231613159\n",
      "Iteration 155100/200000, loss=0.7160018682479858\n",
      "Iteration 155200/200000, loss=0.6703318357467651\n",
      "Iteration 155300/200000, loss=0.6784307360649109\n",
      "Iteration 155400/200000, loss=0.6723775863647461\n",
      "Iteration 155500/200000, loss=0.6909247636795044\n",
      "Iteration 155600/200000, loss=0.6972183585166931\n",
      "Iteration 155700/200000, loss=0.646936297416687\n",
      "Iteration 155800/200000, loss=0.6637073755264282\n",
      "Iteration 155900/200000, loss=0.6788741946220398\n",
      "Iteration 156000/200000, loss=0.6975358128547668\n",
      "Iteration 156100/200000, loss=0.7156985402107239\n",
      "Iteration 156200/200000, loss=0.6815261840820312\n",
      "Iteration 156300/200000, loss=0.7128364443778992\n",
      "Iteration 156400/200000, loss=0.6674404144287109\n",
      "Iteration 156500/200000, loss=0.7227123975753784\n",
      "Iteration 156600/200000, loss=0.6958968639373779\n",
      "Iteration 156700/200000, loss=0.7202403545379639\n",
      "Iteration 156800/200000, loss=0.7800203561782837\n",
      "Iteration 156900/200000, loss=0.675628662109375\n",
      "Iteration 157000/200000, loss=0.6489525437355042\n",
      "Iteration 157100/200000, loss=0.6654697060585022\n",
      "Iteration 157200/200000, loss=0.6591243743896484\n",
      "Iteration 157300/200000, loss=0.6906523704528809\n",
      "Iteration 157400/200000, loss=0.7627003788948059\n",
      "Iteration 157500/200000, loss=0.6581277251243591\n",
      "Iteration 157600/200000, loss=0.7071900963783264\n",
      "Iteration 157700/200000, loss=0.7864750623703003\n",
      "Iteration 157800/200000, loss=0.6749608516693115\n",
      "Iteration 157900/200000, loss=0.6620606780052185\n",
      "Iteration 158000/200000, loss=0.7317783236503601\n",
      "Iteration 158100/200000, loss=0.7345213890075684\n",
      "Iteration 158200/200000, loss=0.7025607228279114\n",
      "Iteration 158300/200000, loss=0.69924396276474\n",
      "Iteration 158400/200000, loss=0.7045542597770691\n",
      "Iteration 158500/200000, loss=0.6629964113235474\n",
      "Iteration 158600/200000, loss=0.7414430379867554\n",
      "Iteration 158700/200000, loss=0.6675485372543335\n",
      "Iteration 158800/200000, loss=0.684977114200592\n",
      "Iteration 158900/200000, loss=0.646432101726532\n",
      "Iteration 159000/200000, loss=0.6867596507072449\n",
      "Iteration 159100/200000, loss=0.6420506834983826\n",
      "Iteration 159200/200000, loss=0.7038444876670837\n",
      "Iteration 159300/200000, loss=0.7890977263450623\n",
      "Iteration 159400/200000, loss=0.6311893463134766\n",
      "Iteration 159500/200000, loss=0.6859135627746582\n",
      "Iteration 159600/200000, loss=0.6779938340187073\n",
      "Iteration 159700/200000, loss=0.6626266241073608\n",
      "Iteration 159800/200000, loss=0.7321860790252686\n",
      "Iteration 159900/200000, loss=0.6797557473182678\n",
      "Iteration 160000/200000, loss=0.7661811113357544\n",
      "Iteration 160100/200000, loss=0.6797277331352234\n",
      "Iteration 160200/200000, loss=0.6344128847122192\n",
      "Iteration 160300/200000, loss=0.735893726348877\n",
      "Iteration 160400/200000, loss=0.7268658876419067\n",
      "Iteration 160500/200000, loss=0.4161371886730194\n",
      "Iteration 160600/200000, loss=0.712763249874115\n",
      "Iteration 160700/200000, loss=0.7319614291191101\n",
      "Iteration 160800/200000, loss=0.7440447211265564\n",
      "Iteration 160900/200000, loss=0.6241925954818726\n",
      "Iteration 161000/200000, loss=0.6837077736854553\n",
      "Iteration 161100/200000, loss=0.6589300632476807\n",
      "Iteration 161200/200000, loss=0.7232734560966492\n",
      "Iteration 161300/200000, loss=0.6830600500106812\n",
      "Iteration 161400/200000, loss=0.7382096648216248\n",
      "Iteration 161500/200000, loss=0.6755486726760864\n",
      "Iteration 161600/200000, loss=0.6953898668289185\n",
      "Iteration 161700/200000, loss=0.7146170735359192\n",
      "Iteration 161800/200000, loss=0.7356305122375488\n",
      "Iteration 161900/200000, loss=0.538867712020874\n",
      "Iteration 162000/200000, loss=0.6836882829666138\n",
      "Iteration 162100/200000, loss=0.6635333299636841\n",
      "Iteration 162200/200000, loss=0.6753857135772705\n",
      "Iteration 162300/200000, loss=0.7032929062843323\n",
      "Iteration 162400/200000, loss=0.7003340125083923\n",
      "Iteration 162500/200000, loss=0.6788979172706604\n",
      "Iteration 162600/200000, loss=0.7258280515670776\n",
      "Iteration 162700/200000, loss=0.7183988094329834\n",
      "Iteration 162800/200000, loss=0.6401621699333191\n",
      "Iteration 162900/200000, loss=0.7100549936294556\n",
      "Iteration 163000/200000, loss=0.7203036546707153\n",
      "Iteration 163100/200000, loss=0.67909175157547\n",
      "Iteration 163200/200000, loss=0.6758595705032349\n",
      "Iteration 163300/200000, loss=0.6991003155708313\n",
      "Iteration 163400/200000, loss=0.6747922897338867\n",
      "Iteration 163500/200000, loss=0.6533670425415039\n",
      "Iteration 163600/200000, loss=0.6759150624275208\n",
      "Iteration 163700/200000, loss=0.6813708543777466\n",
      "Iteration 163800/200000, loss=0.6931092739105225\n",
      "Iteration 163900/200000, loss=0.6704452633857727\n",
      "Iteration 164000/200000, loss=0.6963827013969421\n",
      "Iteration 164100/200000, loss=0.7349337339401245\n",
      "Iteration 164200/200000, loss=0.6829950213432312\n",
      "Iteration 164300/200000, loss=0.6945573091506958\n",
      "Iteration 164400/200000, loss=0.6467422842979431\n",
      "Iteration 164500/200000, loss=0.7421561479568481\n",
      "Iteration 164600/200000, loss=0.6881377100944519\n",
      "Iteration 164700/200000, loss=0.6684046983718872\n",
      "Iteration 164800/200000, loss=0.7015078663825989\n",
      "Iteration 164900/200000, loss=0.6770187020301819\n",
      "Iteration 165000/200000, loss=0.6815083622932434\n",
      "Iteration 165100/200000, loss=0.7125420570373535\n",
      "Iteration 165200/200000, loss=0.7041938900947571\n",
      "Iteration 165300/200000, loss=0.6749085187911987\n",
      "Iteration 165400/200000, loss=0.6943482756614685\n",
      "Iteration 165500/200000, loss=0.639336109161377\n",
      "Iteration 165600/200000, loss=0.639630913734436\n",
      "Iteration 165700/200000, loss=0.6621442437171936\n",
      "Iteration 165800/200000, loss=0.7232685089111328\n",
      "Iteration 165900/200000, loss=0.6635918617248535\n",
      "Iteration 166000/200000, loss=0.6418666243553162\n",
      "Iteration 166100/200000, loss=0.7416585683822632\n",
      "Iteration 166200/200000, loss=0.6762354969978333\n",
      "Iteration 166300/200000, loss=0.7225453853607178\n",
      "Iteration 166400/200000, loss=0.7397011518478394\n",
      "Iteration 166500/200000, loss=0.5945255160331726\n",
      "Iteration 166600/200000, loss=0.6349809169769287\n",
      "Iteration 166700/200000, loss=0.7201337218284607\n",
      "Iteration 166800/200000, loss=0.6942492127418518\n",
      "Iteration 166900/200000, loss=0.6888214349746704\n",
      "Iteration 167000/200000, loss=0.7296737432479858\n",
      "Iteration 167100/200000, loss=0.6691011190414429\n",
      "Iteration 167200/200000, loss=0.6317723989486694\n",
      "Iteration 167300/200000, loss=0.6858248114585876\n",
      "Iteration 167400/200000, loss=0.6743736863136292\n",
      "Iteration 167500/200000, loss=0.8532383441925049\n",
      "Iteration 167600/200000, loss=0.8314952850341797\n",
      "Iteration 167700/200000, loss=0.7282449007034302\n",
      "Iteration 167800/200000, loss=0.6964183449745178\n",
      "Iteration 167900/200000, loss=0.7066267132759094\n",
      "Iteration 168000/200000, loss=0.700786292552948\n",
      "Iteration 168100/200000, loss=0.6650518774986267\n",
      "Iteration 168200/200000, loss=0.7078445553779602\n",
      "Iteration 168300/200000, loss=0.632595419883728\n",
      "Iteration 168400/200000, loss=0.6829109191894531\n",
      "Iteration 168500/200000, loss=0.6886275410652161\n",
      "Iteration 168600/200000, loss=0.6578457951545715\n",
      "Iteration 168700/200000, loss=0.727708637714386\n",
      "Iteration 168800/200000, loss=0.7580710649490356\n",
      "Iteration 168900/200000, loss=0.5662140846252441\n",
      "Iteration 169000/200000, loss=0.729682445526123\n",
      "Iteration 169100/200000, loss=0.7147105932235718\n",
      "Iteration 169200/200000, loss=0.6003854274749756\n",
      "Iteration 169300/200000, loss=0.7081953287124634\n",
      "Iteration 169400/200000, loss=0.6612932085990906\n",
      "Iteration 169500/200000, loss=0.7774063348770142\n",
      "Iteration 169600/200000, loss=0.6956416964530945\n",
      "Iteration 169700/200000, loss=0.660573422908783\n",
      "Iteration 169800/200000, loss=0.664336621761322\n",
      "Iteration 169900/200000, loss=0.6873189210891724\n",
      "Iteration 170000/200000, loss=0.7128340601921082\n",
      "Iteration 170100/200000, loss=0.7575312852859497\n",
      "Iteration 170200/200000, loss=0.6587998867034912\n",
      "Iteration 170300/200000, loss=0.6685042381286621\n",
      "Iteration 170400/200000, loss=0.6569625735282898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 170500/200000, loss=0.7183527946472168\n",
      "Iteration 170600/200000, loss=0.706599771976471\n",
      "Iteration 170700/200000, loss=0.7111468315124512\n",
      "Iteration 170800/200000, loss=0.7081155180931091\n",
      "Iteration 170900/200000, loss=0.6652135252952576\n",
      "Iteration 171000/200000, loss=0.7459118366241455\n",
      "Iteration 171100/200000, loss=0.6620849370956421\n",
      "Iteration 171200/200000, loss=0.7035768628120422\n",
      "Iteration 171300/200000, loss=0.6725472807884216\n",
      "Iteration 171400/200000, loss=0.7648245096206665\n",
      "Iteration 171500/200000, loss=0.7112056612968445\n",
      "Iteration 171600/200000, loss=0.6317949295043945\n",
      "Iteration 171700/200000, loss=0.7020347118377686\n",
      "Iteration 171800/200000, loss=0.6496875286102295\n",
      "Iteration 171900/200000, loss=0.6714088916778564\n",
      "Iteration 172000/200000, loss=0.6712769269943237\n",
      "Iteration 172100/200000, loss=0.7355681657791138\n",
      "Iteration 172200/200000, loss=0.6328980326652527\n",
      "Iteration 172300/200000, loss=0.7945435047149658\n",
      "Iteration 172400/200000, loss=0.7320613861083984\n",
      "Iteration 172500/200000, loss=0.6912245154380798\n",
      "Iteration 172600/200000, loss=0.7731390595436096\n",
      "Iteration 172700/200000, loss=0.6324995160102844\n",
      "Iteration 172800/200000, loss=0.7850505113601685\n",
      "Iteration 172900/200000, loss=0.6982830762863159\n",
      "Iteration 173000/200000, loss=0.6634858846664429\n",
      "Iteration 173100/200000, loss=0.7276306748390198\n",
      "Iteration 173200/200000, loss=0.676278293132782\n",
      "Iteration 173300/200000, loss=0.6933104395866394\n",
      "Iteration 173400/200000, loss=0.7059621214866638\n",
      "Iteration 173500/200000, loss=0.8368366360664368\n",
      "Iteration 173600/200000, loss=0.6752246618270874\n",
      "Iteration 173700/200000, loss=0.6147667765617371\n",
      "Iteration 173800/200000, loss=0.7261495590209961\n",
      "Iteration 173900/200000, loss=0.7427995800971985\n",
      "Iteration 174000/200000, loss=0.661888837814331\n",
      "Iteration 174100/200000, loss=0.6816100478172302\n",
      "Iteration 174200/200000, loss=0.7134239673614502\n",
      "Iteration 174300/200000, loss=0.7575016021728516\n",
      "Iteration 174400/200000, loss=0.7015750408172607\n",
      "Iteration 174500/200000, loss=0.6338717341423035\n",
      "Iteration 174600/200000, loss=0.6474277377128601\n",
      "Iteration 174700/200000, loss=0.6897293925285339\n",
      "Iteration 174800/200000, loss=0.7418477535247803\n",
      "Iteration 174900/200000, loss=0.6595065593719482\n",
      "Iteration 175000/200000, loss=0.7133218050003052\n",
      "Iteration 175100/200000, loss=0.6629142761230469\n",
      "Iteration 175200/200000, loss=0.6379634141921997\n",
      "Iteration 175300/200000, loss=0.7374811172485352\n",
      "Iteration 175400/200000, loss=0.7398934364318848\n",
      "Iteration 175500/200000, loss=0.6507406830787659\n",
      "Iteration 175600/200000, loss=0.6981708407402039\n",
      "Iteration 175700/200000, loss=0.6617482900619507\n",
      "Iteration 175800/200000, loss=0.6641499400138855\n",
      "Iteration 175900/200000, loss=0.6518591046333313\n",
      "Iteration 176000/200000, loss=0.4339126944541931\n",
      "Iteration 176100/200000, loss=0.6837691068649292\n",
      "Iteration 176200/200000, loss=0.7564517855644226\n",
      "Iteration 176300/200000, loss=0.6488003134727478\n",
      "Iteration 176400/200000, loss=0.6751427054405212\n",
      "Iteration 176500/200000, loss=0.6999025344848633\n",
      "Iteration 176600/200000, loss=0.6210556030273438\n",
      "Iteration 176700/200000, loss=0.640652060508728\n",
      "Iteration 176800/200000, loss=0.7405985593795776\n",
      "Iteration 176900/200000, loss=0.6532753705978394\n",
      "Iteration 177000/200000, loss=0.6698009371757507\n",
      "Iteration 177100/200000, loss=0.6833608746528625\n",
      "Iteration 177200/200000, loss=0.6205424070358276\n",
      "Iteration 177300/200000, loss=0.6737553477287292\n",
      "Iteration 177400/200000, loss=0.6795227527618408\n",
      "Iteration 177500/200000, loss=0.7459874153137207\n",
      "Iteration 177600/200000, loss=0.6701226830482483\n",
      "Iteration 177700/200000, loss=0.7123188972473145\n",
      "Iteration 177800/200000, loss=0.6708905696868896\n",
      "Iteration 177900/200000, loss=0.7999593019485474\n",
      "Iteration 178000/200000, loss=0.5653496980667114\n",
      "Iteration 178100/200000, loss=0.7221309542655945\n",
      "Iteration 178200/200000, loss=0.6958039402961731\n",
      "Iteration 178300/200000, loss=0.6697760820388794\n",
      "Iteration 178400/200000, loss=0.6649981737136841\n",
      "Iteration 178500/200000, loss=0.7055808901786804\n",
      "Iteration 178600/200000, loss=0.6551982760429382\n",
      "Iteration 178700/200000, loss=0.6835819482803345\n",
      "Iteration 178800/200000, loss=0.682682454586029\n",
      "Iteration 178900/200000, loss=0.6764976978302002\n",
      "Iteration 179000/200000, loss=0.7441972494125366\n",
      "Iteration 179100/200000, loss=0.7276607751846313\n",
      "Iteration 179200/200000, loss=0.6645612120628357\n",
      "Iteration 179300/200000, loss=0.6470509767532349\n",
      "Iteration 179400/200000, loss=0.6794282793998718\n",
      "Iteration 179500/200000, loss=0.6965188384056091\n",
      "Iteration 179600/200000, loss=0.719238817691803\n",
      "Iteration 179700/200000, loss=0.7284116744995117\n",
      "Iteration 179800/200000, loss=0.7446468472480774\n",
      "Iteration 179900/200000, loss=0.6061590313911438\n",
      "Iteration 180000/200000, loss=0.6976799368858337\n",
      "Iteration 180100/200000, loss=0.7633454203605652\n",
      "Iteration 180200/200000, loss=0.6628955006599426\n",
      "Iteration 180300/200000, loss=0.6654279828071594\n",
      "Iteration 180400/200000, loss=0.6930158138275146\n",
      "Iteration 180500/200000, loss=0.6287711262702942\n",
      "Iteration 180600/200000, loss=0.7149590253829956\n",
      "Iteration 180700/200000, loss=0.6796971559524536\n",
      "Iteration 180800/200000, loss=0.668142557144165\n",
      "Iteration 180900/200000, loss=0.6830922365188599\n",
      "Iteration 181000/200000, loss=0.7000768184661865\n",
      "Iteration 181100/200000, loss=0.6933239102363586\n",
      "Iteration 181200/200000, loss=0.657086968421936\n",
      "Iteration 181300/200000, loss=0.6994614601135254\n",
      "Iteration 181400/200000, loss=0.6785864233970642\n",
      "Iteration 181500/200000, loss=0.6842601895332336\n",
      "Iteration 181600/200000, loss=0.7386879324913025\n",
      "Iteration 181700/200000, loss=0.6881995797157288\n",
      "Iteration 181800/200000, loss=0.7614073753356934\n",
      "Iteration 181900/200000, loss=0.7281830906867981\n",
      "Iteration 182000/200000, loss=0.6779154539108276\n",
      "Iteration 182100/200000, loss=0.7429541945457458\n",
      "Iteration 182200/200000, loss=0.6933581829071045\n",
      "Iteration 182300/200000, loss=0.6642318964004517\n",
      "Iteration 182400/200000, loss=0.7373067140579224\n",
      "Iteration 182500/200000, loss=0.7014913558959961\n",
      "Iteration 182600/200000, loss=0.7086034417152405\n",
      "Iteration 182700/200000, loss=0.6999576091766357\n",
      "Iteration 182800/200000, loss=0.6656065583229065\n",
      "Iteration 182900/200000, loss=0.6902435421943665\n",
      "Iteration 183000/200000, loss=0.6625850796699524\n",
      "Iteration 183100/200000, loss=0.6385741233825684\n",
      "Iteration 183200/200000, loss=0.6886917948722839\n",
      "Iteration 183300/200000, loss=0.7230378985404968\n",
      "Iteration 183400/200000, loss=0.735437273979187\n",
      "Iteration 183500/200000, loss=0.6830769777297974\n",
      "Iteration 183600/200000, loss=0.6595756411552429\n",
      "Iteration 183700/200000, loss=0.656114399433136\n",
      "Iteration 183800/200000, loss=0.6351014971733093\n",
      "Iteration 183900/200000, loss=0.6490674614906311\n",
      "Iteration 184000/200000, loss=0.7214301228523254\n",
      "Iteration 184100/200000, loss=0.680103063583374\n",
      "Iteration 184200/200000, loss=0.6973804235458374\n",
      "Iteration 184300/200000, loss=0.7349941730499268\n",
      "Iteration 184400/200000, loss=0.739081621170044\n",
      "Iteration 184500/200000, loss=0.6805639863014221\n",
      "Iteration 184600/200000, loss=0.676216721534729\n",
      "Iteration 184700/200000, loss=0.6985999941825867\n",
      "Iteration 184800/200000, loss=0.7367996573448181\n",
      "Iteration 184900/200000, loss=0.6501986384391785\n",
      "Iteration 185000/200000, loss=0.6784173250198364\n",
      "Iteration 185100/200000, loss=0.7495803833007812\n",
      "Iteration 185200/200000, loss=0.6010984182357788\n",
      "Iteration 185300/200000, loss=0.6460330486297607\n",
      "Iteration 185400/200000, loss=0.524681568145752\n",
      "Iteration 185500/200000, loss=0.7010201811790466\n",
      "Iteration 185600/200000, loss=0.7572991847991943\n",
      "Iteration 185700/200000, loss=0.6552000641822815\n",
      "Iteration 185800/200000, loss=0.7264166474342346\n",
      "Iteration 185900/200000, loss=0.670072615146637\n",
      "Iteration 186000/200000, loss=0.5832642316818237\n",
      "Iteration 186100/200000, loss=0.6985806822776794\n",
      "Iteration 186200/200000, loss=0.6602683067321777\n",
      "Iteration 186300/200000, loss=0.6252195835113525\n",
      "Iteration 186400/200000, loss=0.6706646680831909\n",
      "Iteration 186500/200000, loss=0.6609715223312378\n",
      "Iteration 186600/200000, loss=0.6972343921661377\n",
      "Iteration 186700/200000, loss=0.5746266841888428\n",
      "Iteration 186800/200000, loss=0.6555611491203308\n",
      "Iteration 186900/200000, loss=0.675430417060852\n",
      "Iteration 187000/200000, loss=0.7150444984436035\n",
      "Iteration 187100/200000, loss=0.6838845014572144\n",
      "Iteration 187200/200000, loss=0.6471489667892456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 187300/200000, loss=0.6541406512260437\n",
      "Iteration 187400/200000, loss=0.7873542904853821\n",
      "Iteration 187500/200000, loss=0.7334285974502563\n",
      "Iteration 187600/200000, loss=0.5279873609542847\n",
      "Iteration 187700/200000, loss=0.7084088325500488\n",
      "Iteration 187800/200000, loss=0.6772411465644836\n",
      "Iteration 187900/200000, loss=0.7371497750282288\n",
      "Iteration 188000/200000, loss=0.8274521827697754\n",
      "Iteration 188100/200000, loss=0.6908352971076965\n",
      "Iteration 188200/200000, loss=0.7606729865074158\n",
      "Iteration 188300/200000, loss=0.7443906664848328\n",
      "Iteration 188400/200000, loss=0.814255952835083\n",
      "Iteration 188500/200000, loss=0.6987701654434204\n",
      "Iteration 188600/200000, loss=0.6985394358634949\n",
      "Iteration 188700/200000, loss=0.6753919720649719\n",
      "Iteration 188800/200000, loss=0.6835858225822449\n",
      "Iteration 188900/200000, loss=0.6834619641304016\n",
      "Iteration 189000/200000, loss=0.6651592254638672\n",
      "Iteration 189100/200000, loss=0.6604185104370117\n",
      "Iteration 189200/200000, loss=0.649382472038269\n",
      "Iteration 189300/200000, loss=0.6954219937324524\n",
      "Iteration 189400/200000, loss=0.7278304100036621\n",
      "Iteration 189500/200000, loss=0.6835975050926208\n",
      "Iteration 189600/200000, loss=0.6853553056716919\n",
      "Iteration 189700/200000, loss=0.6648637056350708\n",
      "Iteration 189800/200000, loss=0.7460297346115112\n",
      "Iteration 189900/200000, loss=0.6637728810310364\n",
      "Iteration 190000/200000, loss=0.7109520435333252\n",
      "Iteration 190100/200000, loss=0.6332088112831116\n",
      "Iteration 190200/200000, loss=0.7030112147331238\n",
      "Iteration 190300/200000, loss=0.71075040102005\n",
      "Iteration 190400/200000, loss=0.7428743839263916\n",
      "Iteration 190500/200000, loss=0.6147238612174988\n",
      "Iteration 190600/200000, loss=0.626841127872467\n",
      "Iteration 190700/200000, loss=0.6761679649353027\n",
      "Iteration 190800/200000, loss=0.6422403454780579\n",
      "Iteration 190900/200000, loss=0.6934979557991028\n",
      "Iteration 191000/200000, loss=0.670425295829773\n",
      "Iteration 191100/200000, loss=0.7213496565818787\n",
      "Iteration 191200/200000, loss=0.6808726787567139\n",
      "Iteration 191300/200000, loss=0.6672883629798889\n",
      "Iteration 191400/200000, loss=0.6983324885368347\n",
      "Iteration 191500/200000, loss=0.72581547498703\n",
      "Iteration 191600/200000, loss=0.7074996829032898\n",
      "Iteration 191700/200000, loss=0.6315721273422241\n",
      "Iteration 191800/200000, loss=0.6611440181732178\n",
      "Iteration 191900/200000, loss=0.6725978851318359\n",
      "Iteration 192000/200000, loss=0.7929259538650513\n",
      "Iteration 192100/200000, loss=0.6481761932373047\n",
      "Iteration 192200/200000, loss=0.7192548513412476\n",
      "Iteration 192300/200000, loss=0.6854892373085022\n",
      "Iteration 192400/200000, loss=0.5877563953399658\n",
      "Iteration 192500/200000, loss=0.7110713124275208\n",
      "Iteration 192600/200000, loss=0.7491228580474854\n",
      "Iteration 192700/200000, loss=0.6527901887893677\n",
      "Iteration 192800/200000, loss=0.7999756336212158\n",
      "Iteration 192900/200000, loss=0.6624910831451416\n",
      "Iteration 193000/200000, loss=0.6845236420631409\n",
      "Iteration 193100/200000, loss=0.6537341475486755\n",
      "Iteration 193200/200000, loss=0.683236300945282\n",
      "Iteration 193300/200000, loss=0.6303529739379883\n",
      "Iteration 193400/200000, loss=0.7627322673797607\n",
      "Iteration 193500/200000, loss=0.6334115862846375\n",
      "Iteration 193600/200000, loss=0.6991092562675476\n",
      "Iteration 193700/200000, loss=0.7918182611465454\n",
      "Iteration 193800/200000, loss=0.7099230885505676\n",
      "Iteration 193900/200000, loss=0.6958320736885071\n",
      "Iteration 194000/200000, loss=0.6701495051383972\n",
      "Iteration 194100/200000, loss=0.6986127495765686\n",
      "Iteration 194200/200000, loss=0.7059291005134583\n",
      "Iteration 194300/200000, loss=0.7145031094551086\n",
      "Iteration 194400/200000, loss=0.6924883723258972\n",
      "Iteration 194500/200000, loss=0.6859201192855835\n",
      "Iteration 194600/200000, loss=0.7290494441986084\n",
      "Iteration 194700/200000, loss=0.6318097114562988\n",
      "Iteration 194800/200000, loss=0.7658493518829346\n",
      "Iteration 194900/200000, loss=0.7396103143692017\n",
      "Iteration 195000/200000, loss=0.704060435295105\n",
      "Iteration 195100/200000, loss=0.6258683800697327\n",
      "Iteration 195200/200000, loss=0.6919827461242676\n",
      "Iteration 195300/200000, loss=0.7869938611984253\n",
      "Iteration 195400/200000, loss=0.6946772933006287\n",
      "Iteration 195500/200000, loss=0.728098452091217\n",
      "Iteration 195600/200000, loss=0.6072047352790833\n",
      "Iteration 195700/200000, loss=0.6712884902954102\n",
      "Iteration 195800/200000, loss=0.6444721817970276\n",
      "Iteration 195900/200000, loss=0.8031145930290222\n",
      "Iteration 196000/200000, loss=0.6783469319343567\n",
      "Iteration 196100/200000, loss=0.7011069655418396\n",
      "Iteration 196200/200000, loss=0.5759738087654114\n",
      "Iteration 196300/200000, loss=0.6866464018821716\n",
      "Iteration 196400/200000, loss=0.6261584162712097\n",
      "Iteration 196500/200000, loss=0.7533197402954102\n",
      "Iteration 196600/200000, loss=0.7117217183113098\n",
      "Iteration 196700/200000, loss=0.6826388239860535\n",
      "Iteration 196800/200000, loss=0.6827508211135864\n",
      "Iteration 196900/200000, loss=0.6445183157920837\n",
      "Iteration 197000/200000, loss=0.7450454235076904\n",
      "Iteration 197100/200000, loss=0.7374207973480225\n",
      "Iteration 197200/200000, loss=0.6851625442504883\n",
      "Iteration 197300/200000, loss=0.6959971189498901\n",
      "Iteration 197400/200000, loss=0.7233863472938538\n",
      "Iteration 197500/200000, loss=0.7128679156303406\n",
      "Iteration 197600/200000, loss=0.718500018119812\n",
      "Iteration 197700/200000, loss=0.7393875122070312\n",
      "Iteration 197800/200000, loss=0.7521887421607971\n",
      "Iteration 197900/200000, loss=0.6829984784126282\n",
      "Iteration 198000/200000, loss=0.6806565523147583\n",
      "Iteration 198100/200000, loss=0.649522602558136\n",
      "Iteration 198200/200000, loss=0.7811813354492188\n",
      "Iteration 198300/200000, loss=0.6965314149856567\n",
      "Iteration 198400/200000, loss=0.6899983286857605\n",
      "Iteration 198500/200000, loss=0.6499636173248291\n",
      "Iteration 198600/200000, loss=0.6643500328063965\n",
      "Iteration 198700/200000, loss=0.6895021796226501\n",
      "Iteration 198800/200000, loss=0.6698325276374817\n",
      "Iteration 198900/200000, loss=0.661411702632904\n",
      "Iteration 199000/200000, loss=0.6718264222145081\n",
      "Iteration 199100/200000, loss=0.7215296030044556\n",
      "Iteration 199200/200000, loss=0.63176029920578\n",
      "Iteration 199300/200000, loss=0.701317548751831\n",
      "Iteration 199400/200000, loss=0.7153924107551575\n",
      "Iteration 199500/200000, loss=0.6969316601753235\n",
      "Iteration 199600/200000, loss=0.7042465806007385\n",
      "Iteration 199700/200000, loss=0.6688559055328369\n",
      "Iteration 199800/200000, loss=0.6320173740386963\n",
      "Iteration 199900/200000, loss=0.6313409209251404\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "def build_model(n_movies, embedding_size=300):\n",
    "    input_target = Input((1,))\n",
    "    input_context = Input((1,))\n",
    "\n",
    "    embedding = Embedding(n_movies, embedding_size, input_length=1, name='embedding')\n",
    "    target = embedding(input_target)\n",
    "    target = Reshape((embedding_size, 1))(target)\n",
    "    context = embedding(input_context)\n",
    "    context = Reshape((embedding_size, 1))(context)\n",
    "\n",
    "    x = Dot(axes=1)([target, context])\n",
    "    x = Reshape((1,))(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=[input_target, input_context], outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    similarity = Dot(axes=0, normalize=True)([target, context])\n",
    "    similarity_model = Model(inputs=[input_target, input_context], outputs=similarity)\n",
    "    \n",
    "    return model, similarity_model\n",
    "\n",
    "def get_train_pairs(train_user_movie_seq, val_user_movie_seq, sampling_table, window_size=2):\n",
    "    targets, contexts, labels = [], [], []\n",
    "    for movie_seq1, movie_seq2 in zip(train_user_movie_seq, val_user_movie_seq):\n",
    "        movie_seq = movie_seq1 + movie_seq2\n",
    "        \n",
    "        pairs = []\n",
    "        while (len(pairs) == 0):\n",
    "            pairs, pairs_labels = skipgrams(movie_seq, len(sampling_table), window_size=window_size,\n",
    "                                            sampling_table=sampling_table)\n",
    "        target, context = zip(*pairs)\n",
    "\n",
    "        targets += target\n",
    "        contexts += context\n",
    "        labels += pairs_labels\n",
    "        \n",
    "    targets = np.array(targets, dtype=\"int32\")\n",
    "    contexts = np.array(contexts, dtype=\"int32\")\n",
    "    \n",
    "    return targets, contexts, labels\n",
    "\n",
    "model, similarity_model = build_model(n_movies)\n",
    "sampling_table = sequence.make_sampling_table(n_movies)\n",
    "\n",
    "# n_epochs = 1\n",
    "# for _epoch in range(epochs):\n",
    "#     # target ~= ancor for window\n",
    "#     targets, contexts, labels = get_train_pairs(train_user_movie_seq, val_user_movie_seq, sampling_table)\n",
    "#     model.fit([targets, contexts], labels, batch_size=1)\n",
    "\n",
    "epochs = 200000\n",
    "placeholder_1 = np.zeros((1,))\n",
    "placeholder_2 = np.zeros((1,))\n",
    "placeholder_3 = np.zeros((1,))\n",
    "targets, contexts, labels = get_train_pairs(train_user_movie_seq, val_user_movie_seq, sampling_table)\n",
    "for cnt in range(epochs):\n",
    "    ind = np.random.randint(0, len(labels)-1)\n",
    "    placeholder_1[0,] = targets[ind]\n",
    "    placeholder_2[0,] = contexts[ind]\n",
    "    placeholder_3[0,] = labels[ind]\n",
    "    loss = model.train_on_batch([placeholder_1, placeholder_2], placeholder_3)\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"Iteration {}/{}, loss={}\".format(cnt, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('data/model.h5')\n",
    "similarity_model.save_weights('data/similarity_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "получаем матрицу S расстояний между фильмами, так как в статье отмечено, что она вычислена заранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.eye(n_movies)\n",
    "for i in range(n_movies):\n",
    "    input_1 = np.zeros((1,))\n",
    "    input_1[0,] = i\n",
    "    for j in range(i + 1, n_movies):\n",
    "        input_2 = np.zeros((1,))\n",
    "        input_2[0,] = j\n",
    "        \n",
    "        cos_dist = similarity_model.predict_on_batch([input_1, input_2])\n",
    "        S[i, j] = S[j, i] = cos_dist\n",
    "        \n",
    "np.save('data/SGNS_dists.npy', S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization using learned S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "не успел реализовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MF_SGNS:\n",
    "    # d - size of latent vectors\n",
    "    def __init__(self, user_ratings, user_movie_seq, S, n_movies, n_iter=20, d=128, k=5, a=0.5, l=0.1, lr=0.001):\n",
    "        self.k = k\n",
    "        self.a = a\n",
    "        self.l = l \n",
    "        self.d = d\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.__build_R(user_ratings, user_movie_seq, n_movies)\n",
    "        self.n_movies = R.shape[1]\n",
    "        self.S = S        \n",
    "        self.P = np.random.normal(scale=1/d, size=(len(R), d))\n",
    "        self.Q = np.random.normal(scale=1/d, size=(n_movies, d))\n",
    "        self.b_u = np.zeros(len(R))\n",
    "        self.b_i = np.zeros(n_movies)\n",
    "        self.m = np.mean(self.R[np.where(self.R != 0)])      \n",
    "            \n",
    "        def train(self):\n",
    "            for i in range(self.n_iter):\n",
    "                self.__sgd()\n",
    "                mse = np.sqrt(np.sum((self.R - self.__get_R_pred()) ** 2))\n",
    "                print(\"Iteration {}. mse: {}\".format(i, mse))\n",
    "                    \n",
    "        def predict_r(self, i, j):\n",
    "            return self.m + self.b_u[i] + self.b_i[j] + self.P[i, :].dot(self.Q[j, :].T)           \n",
    "\n",
    "        def __build_R(user_ratings, user_movie_seq, n_movies):\n",
    "            self.R = np.zeros((len(user_ratings), n_movies))\n",
    "            for user_id, (ratings, movies) in enumerate(zip(user_ratings, user_movie_seq)):\n",
    "                self.R[user_id, movies] = ratings          \n",
    "        \n",
    "        def __sgd(self):\n",
    "                pass\n",
    "                \n",
    "        def __get_R_pred(self):\n",
    "            return self.m + self.b_u[:,np.newaxis] + self.b_i + self.P.dot(self.Q.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
